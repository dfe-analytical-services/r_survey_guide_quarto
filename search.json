[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SuRvey analysis guide in R",
    "section": "",
    "text": "Introduction\nThis is a guide on how to carry out routine analysis that is usually carried out in SPSS but in R. All the instructions for this guide are on this website and the code is in the scripts in the repository linked on this page in the Get the code section. It includes sections about how to:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "SuRvey analysis guide in R",
    "section": "",
    "text": "set up your workspace\nread in and label survey data that is in SPSS or CSV format\nformat, filter data and view data dictionaries\ncreate frequencies\ncreate crosstabs\ncreate summary tables for numerical data\ntest for significance\ntest for correlation\nchange working directories - optional\nstart working with Git - optional",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#user-guide",
    "href": "index.html#user-guide",
    "title": "SuRvey analysis guide in R",
    "section": "User guide",
    "text": "User guide\nThere are different ways to use this guide. You can:\n\nuse it to learn how to do survey analysis in R.\nuse it for your own projects\n\n\n\n\n\n\n\nEssential steps\n\n\n\nTo use this guide to learn or for your own projects, you MUST complete steps 1, 2 and 3 below.\n\n\n\nStep 1 - Download required software\n\nOpen Software Center on your laptop. You can navigate to it by searching “Software Center” in the search bar in the task bar.\nDownload the following software:\n\nR for Windows 4.4 or higher\nRStudio\nRTools 4.4 or higher\n\n\n\n\n\n\n\n\nSoftware Versions\n\n\n\nIt is important that all your software needs to be the same version. For example, if R is 4.4, your RTools also needs to be 4.4.\nYou can find the version number under the name of the software in Software Center or by clicking on the software and looking for “Version:”  in the information.\nYou can also check the R version you’re using in RStudio by checking the number under the console tab as shown in the image below. You can also use the function getRversion() in the console.\n\n\n\n\n\n\n\n\n\nSwitching R version\n\n\n\nIf you have the latest version of R downloaded but RStudio still shows an older one, use this guide to help you switch to the version of R you need.\n\n\n\n\nStep 2 - Get the code\nFollow the instructions below to download the scripts required for this guide.\n\nGo to the repo.\nClick on “Code”.\nClick on “Download Zip”.\n\n\n\nFind the zipped file where you downloaded it and right click it. Then click “Extract all”.\n\n\n\nClick “Browse”.\n\n\n\nClick on the file path in the pop-up window and delete everything up to your username. This changes the location to outside of your OneDrive.\n\nSaving R Projects outside OneDrive prevents any errors that happen with OneDrive permissions when downloading packages.\n\n\n\n\nClick “New Folder” and name your folder something like survey_analysis and then click “Select Folder”.\n\n\n\nClick “Extract”\n\n\nThis will download an R project with scripts that are separated into sections similar to the sections in this guide. Those scripts contain similar code to what is in this guide. The code will be saved in the survey_analysis folder or whatever name you chose to give that folder.\nIf you wish to use Git for this guide or your project, please use the Working with Git section for guidance.\n\n\nStep 3 - Opening the R project\nR Project files are a way of managing code for your analysis. It helps maintain a working directory and minimizes errors with package management. The zipped file you downloaded contains the R project file needed for this guide.\nTo open scripts for this guide in a project environment, follow these steps:\n\nOpen the folder your project is in.\nDouble click on the file with an R cube symbol next to it. This file has the file type R Project. You can check this by looking at the file properties.\n\n\n\nWhen the project opens, you should be able to see the scripts in the “Files” pane in RStudio. You should also see the name of your folder at the top right-hand side of RStudio.\n\n - Open the scripts you need from here. If you’re using this for your own project, you can create scripts and save them here.\n\n\nStep 4 - Set up your workspace\nMake sure to set up your workspace in the setting up your workspace section of this guide. The two essential steps from that section are:\n\nSet up renv to download packages in the required versions\nLoad necessary packages and functions\n\n\n\nStep 5a - Use the guide for learning\nTo use the guide for learning, open the script corresponding to the section you’re on and run the pre-written code. After that you can create your own script in the project and write your own code variations and data using the outlined functions in the section to test your knowledge.\n\n\nStep 5b - Use the guide for your own project\n\nDelete all the scripts except for the 00_packages_and_functions.R script.\n\nYou can always go back to the repository and re-download the scripts for reference.\n\nRename your R Project file and the folder the project is in to reflect your own project.\nCreate your own scripts and use the functions you want for them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#things-to-be-aware-of",
    "href": "index.html#things-to-be-aware-of",
    "title": "SuRvey analysis guide in R",
    "section": "Things to be aware of",
    "text": "Things to be aware of\n\nWeights\nSPSS calculates weighted results slightly differently to R. SPSS rounds weighted counts to the nearest whole number, and then bases any subsequent statistics off that.\nR does not round, so results are slightly more accurate. You may find that results do not exactly match SPSS output (difference of &lt;0.5pp) depending on how your contractor has calculated their weights.\nGet more information about weights here.\n\n\nCustom functions\nWe created custom functions to make using packages like survey even more user-friendly. We explain how to use those functions throughout this guide. You can use the Custom functions reference page as a quick guide for those functions.\n\n\nSyntax\nYou will notice in this guide that most of the syntax for functions will follow this format, package::function(). For example, dplyr::mutate() or haven::read_spss(). This means that we are using the mutate() function from the dplyr package and the read_spss() function from the haven package. We use this syntax to ensure that:\n\nthe correct function from the intended package is used.\nusers are aware of which package that function comes from so they can look at documentation when reviewing code.\n\nThis is because different packages can have functions with the same name. If you load those packages together in one project, it is difficult to know which function you are using and you can end up using the wrong one. This can lead to inaccurate results.\n\n\nPackages used in this guide\nIn this guide we use a mix of standard package functions and custom functions. We created custom functions to make using some of the standard functions even easier. These custom functions can be found in 00_packages_and_functions.R We will show examples of how to use both standard and custom functions in this guide.\n\nDplyr\nWe will be using the function dplyr::mutate() from the dplyr package in this guide. We will also be combining it with functions like dplyr::across().\nFind out more about dplyr::mutate().\nFind out more about dplyr::across().\n\n\nSurvey\nThis package is useful for producing:\n\nfrequency tables\ncrosstabs\nsummary tables\nweighted averages\n\nFunctions in this package are part of the custom functions you will be using in this guide.\nFind out more information about survey.\n\n\nsjstats\nThis package contains a collection of functions that we will use for significance testing. We will show examples of how to use them in this guide.\nFind out more information about sjstats.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "setting_up_workspace.html",
    "href": "setting_up_workspace.html",
    "title": "1  Setting up your workspace",
    "section": "",
    "text": "1.1 renv\nYou can use 01_setting_up_workspace.R as a reference for this section.\nThis section covers how to set up your R workspace. The renv and loading in packages and functions sections are crucial for running any functions mentioned in this guide.\nrenv is a package that helps you create reproducible environments for your R projects. It gives you a separate library for each project. This gives you the benefits of isolation: different projects can use different versions of packages, and installing, updating, or removing packages in one project doesn’t affect any other project.\nThis is important because packages get updated by their creators all the time and the functions in those packages may be updated in a way that would stop our code from working as it is currently written. Using renv allows us to run our processes without the risk of our code not working because of package updates. This gives us time to work on updating code without having to rush or miss deadlines.\nBelow is how to install all the packages needed for this project in the correct version using renv.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your workspace</span>"
    ]
  },
  {
    "objectID": "setting_up_workspace.html#renv",
    "href": "setting_up_workspace.html#renv",
    "title": "1  Setting up your workspace",
    "section": "",
    "text": "How often do I need to do this?\n\n\n\nThis step only needs to be done ONCE each time a project that uses renv is downloaded, forked or cloned from the repo. You do NOT need to do this every time you open a project.\nYou will know if a project uses renv if you see a renv folder in the project.\n\n\n\n1.1.1 Step 1 - Install the renv package and compare packages installed vs needed\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. Run each line separately.\n\n\n\n#install renv\ninstall.packages(\"renv\")\n\nAfter you run the code above, a message that says \"Do you want to proceed? [Y/n]?\" will come up in the console to confirm if you want to install the renv package. You need to type Y in the console and press enter.\n\n\n\n1.1.2 Step 2 - Download any missing packages\nNow we need to compare the packages you have already to the packages needed by this project. To do this, you need to run the code below.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project.\n\n\n\n#compare installed and required packages\nrenv::restore()\n\nAfter you run the code above, a list of the packages that are not installed at all or not installed in the correct version will appear in the console. This is followed by a message that says \"Do you want to proceed? [Y/n]?\". You need to type Y and press enter to download the required in the correct versions. An example of this message is shown in the image below.\n\n\n\n\n\n\n\nOptional - updating renv\n\n\n\n\n\nIf you are using these scripts for a project and you’re using more packages and you want to update renv, then follow these instructions. Type renv::status() in the console to find out what those packages are. If you want to add those packages to your renv, use renv::snapshot().\n\n\n\n\n\n1.1.3 Extra renv guidance\nRead this documentation for more information on the renv package and how it works.\nRead this section of the DfE Analyst Guide to find out more about renv and ways to troubleshoot common errors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your workspace</span>"
    ]
  },
  {
    "objectID": "setting_up_workspace.html#loading-in-packages-and-functions",
    "href": "setting_up_workspace.html#loading-in-packages-and-functions",
    "title": "1  Setting up your workspace",
    "section": "1.2 Loading in packages and functions",
    "text": "1.2 Loading in packages and functions\nWe are going to use multiple packages and custom functions. Packages contain functions pre-made to help us with analytical tasks. Custom functions are ones that are made by us to help streamline repetitive tasks and provide an easier approach.\n\n\n\n\n\n\nImportant\n\n\n\nYou need to load these in every time you:\n\nrestart your project\nclose and open your project\nclear your environment\n\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project\n\n\n\n#load in packages and functions \n\nsource(\"00_packages_and_functions.R\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up your workspace</span>"
    ]
  },
  {
    "objectID": "reading_labelling_data.html",
    "href": "reading_labelling_data.html",
    "title": "2  Reading and labelling data",
    "section": "",
    "text": "2.1 Dummy data\nYou can use 02_reading_labelling_data.R as a reference for this section.\nThis section will cover how to read and label survey analysis data. This will include data in SPSS and CSV format.\nThe data we will be using in this guide will be dummy data. This means that the data sets we are using will have similar layouts to data you might be familiar with when it comes to survey analysis, but they do not contain real records.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and labelling data</span>"
    ]
  },
  {
    "objectID": "reading_labelling_data.html#spss-data",
    "href": "reading_labelling_data.html#spss-data",
    "title": "2  Reading and labelling data",
    "section": "2.2 SPSS data",
    "text": "2.2 SPSS data\n\n2.2.1 Reading SPSS data\nIn this example, we will load data from a file called TELS_DUMMY_DATA from the data folder in our R project by using the haven::read_spss() function. We need to include the file extension in the name (i.e., .sav in the case of an SPSS file).\nFor your data in your own project, you need to replace the file name.\n\nThings to note:\n\nDepending on the file size, this step can take a few minutes to run.\nMetadata like value labels are read into background classes (e.g., factors) in R that can be called by code for use in analysis.\nuser_na = FALSE ensures that user-defined missing values in the SPSS file are read into R as NA.\n\nIf you want to bring in these codes into analysis (e.g., you might be interested in the share who refused) change to TRUE.\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project.\n\n\n\n#read in data from the data folder using the haven package\nTELSdata &lt;- haven::read_spss(\"data/TELS_DUMMY_DATA.sav\", user_na=FALSE)\n\n\n\n\n2.2.2 Labelling SPSS data\nWhen you load SPSS data using haven as in the previous section, value labels are stored in the background as ‘factors’. This allows you to re-code data using the values and produce statistical outputs using the labels, which is preferred.\nHowever, sometimes it is useful to work with the labels directly (i.e. in SPSS, you can toggle ‘Display value labels’ in the View tab). This can be useful for producing ‘tidy’ data frames, which can be read by tools like Explore Education Statistics (EES). For instance, we use this format to produce the Employer Skills Survey Official Statistics publication.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#Label SPSS datasets so that you are using labels instead of codes.\n# This dataset format is not used elsewhere in this script.\nTELSdata_labelled &lt;- haven::as_factor(TELSdata)\n\n\n\n2.2.3 Data dictionaries\nThis is a way to display the data dictionary in the RStudio viewer and it provides variable names and value codes/labels, it also calculates an unweighted frequency table for every variable. You can customize this code to add weighted frequencies and percentages for your variables as well.\n\n\n\n\n\n\nWarning\n\n\n\nDocumentation for sjPlot::view_df() is not clear on how to display weighted frequencies or percentages. The way to adapt the function is shown below.\n\n\nThis is the working version of the code that displays a data dictionary with variable names and value codes/labels with an unweighted frequency table for every variable. It also shows weighted frequencies and percentages for your variables. If you don’t want to show weighted frequencies or percentages, just omit the following arguments in the function: weight.by, show.wtd.prc and show.wtd.freq.\n\n\n\n\n\n\nConvert weight column to numeric data type\n\n\n\nNotice that we used dplyr::mutate() to convert the weight column TechEd_W3_cross. We did this to ensure that the weight column is recognized by the function. If you omit this step, you may get errors.\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# Display the data dictionary in the R studio viewer\n\n# This will open the dictionary in your browser (which you could save to your folder as html)\n# and also displays the no. of missings for each variable\n\nsjPlot::view_df(TELSdata %&gt;%\n                  #make sure that the weight variable is numeric\n                  dplyr::mutate(TechEd_W3_cross=as.numeric(TechEd_W3_cross)),\n                #show the frequency of each variable\n                show.frq = TRUE,\n                #show the percentage of each variable\n                show.prc = TRUE,\n                #show the number of missings for each variable\n                show.na=TRUE,\n                #show the frequency of each variable weighted by the TechEd_W3_cross variable\n                show.wtd.frq = TRUE,\n                #show the percentage of each variable weighted by the TechEd_W3_cross variable\n                show.wtd.prc = TRUE,\n                #define weight\n                weight.by = TechEd_W3_cross,\n                #display output in browser instead of viewer pane in RStudio\n                use.viewer=FALSE\n)\n\nIf you wanted to search for a variable based on the label, you could use the following function. Just type the key words you’re looking for and it will return the variables in the console.\nBelow, we are going to search for questions that have the word ‘reas’ in the label.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\nTELSdata %&gt;%\n  labelled::look_for(\"reas\")\n\n\n\n2.2.4 Alternative data dictionary\nUnlike the dictionary above, this approach enables you to create a data frame, which can be exported to Excel.\nYou can do this in wide or long formats as shown in the code below. The first example shows the wide format version, and the latter example shows the long format version. Long format can be better for interrogating data since each value label is assigned its own row in the table.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# data dictionary in a wide format\n# create a dataframe, which can be exported to Excel\n\nTELSdata_dic &lt;- TELSdata %&gt;%\n  #create a data dictionary\n  labelled::look_for(details = TRUE) %&gt;%\n  #convert named list columns to character vectors so they're easier to read\n  labelled::convert_list_columns_to_character()\n\n# Saves it as a csv in your work folder\n\ndata.table::fwrite(TELSdata_dic, file=\"TELSdata_dic.csv\")\n\n# If you want to properly interrogate the value labels, it's better to use the long format below\n# since each value label is assigned its own row in the table.\n# Create a dictionary but in a long format \n\nTELSdata_dic2 &lt;- TELSdata %&gt;%\n  #create a data dictionary\n  labelled::look_for(details = TRUE) %&gt;%\n  #convert data to long format\n  labelled::lookfor_to_long_format() %&gt;%\n  #convert named list columns to character vectors so they're easier to read\n  labelled::convert_list_columns_to_character()\n\n# Saves it as a csv in your work folder\ndata.table::fwrite(TELSdata_dic2, file=\"TELSdata_dic2.csv\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and labelling data</span>"
    ]
  },
  {
    "objectID": "reading_labelling_data.html#csv-data",
    "href": "reading_labelling_data.html#csv-data",
    "title": "2  Reading and labelling data",
    "section": "2.3 CSV data",
    "text": "2.3 CSV data\n\n2.3.1 Reading CSV data\nIf you have CSV data, you can read it in using data.table::fread() like the code below, just change the parts that refer to the name of the file to reflect yours.\n\n\n\n\n\n\nNote\n\n\n\nCSV data does not contain labels, so you will need to refer to the questionnaire or data dictionary and add these manually if you want to produce labelled outputs.\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project.\n\n\n\n#Read in the csv data\npplv_data &lt;- data.table::fread(\"data/pplv_dummy_data.csv\")\n\n\n\n2.3.2 Labelling CSV data manually\nSince CSV data does not contain metadata or labels, we can add these manually. This code can also be used for re-labelling data. You can do this by defining the labels for each code and assigning that to your variables.\nFirst, we should check if there are labels in our data.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point, make sure to read in the pplv_data from earlier in this script.\n\n\n\n#check labels in a column before assigning labels to codes- you should just see numbers and NAs\n#you can do any col but we picked futurestudy_consider_bachelors\npplv_data$futurestudy_consider_bachelors\n\nAfter running the code above, you should see just numbers and NAs in the console as shown in the image below.\n\nThen we are going to create vectors to define your labels as shown in the code below.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point, make sure to read in the pplv_data from earlier in this script.\n\n\n\n#define the labels for each variable you want to label \nfuture_study_labels &lt;- c(\"Don't know\" = -1,\n                         \"Very likely\" = 1,\n                         \"Fairly likely\" = 2,\n                         \"Not very likely\" = 3,\n                         \"Not at all likely\" = 4,\n                         \"I've never heard of this type of qualification\" = 5\n)\n\nfuturestudy_HTQ_labels &lt;- c(\"Don't know\" = -1,\n                            \"A lot\" = 1,\n                            \"A little\" = 2,\n                            \"Only heard of the name\" = 3,\n                            \"Never heard of them\" = 4\n)\n\nfuturestudy_plan_labels &lt;- c(\"Don't know\" = -1,\n                             \"Yes - I'm definitely planning on doing this\" = 1,\n                             \"Yes - I'm considering this along with other options\" = 2,\n                             \"No - but I considered this\" = 3,\n                             \"No - and I did not consider this\" = 4,\n                             \"I've never heard of this type of qualification\" = 5\n)\n\nSecond step is to use the dplyr::mutate() function to apply the labels or to relabel your data as shown below.\n\n#mutate the columns for the specific variables to re-label the data\npplv_data &lt;- pplv_data %&gt;%\n  dplyr::mutate(\n    #relabelling futurestudy_consider_bachelors column with future_study_labels\n    futurestudy_consider_bachelors =\n      haven::labelled(futurestudy_consider_bachelors\n                      , labels = future_study_labels\n      ),\n    #relabelling futurestudy_HTQaware_rebase column with futurestudy_HTQ_labels\n    futurestudy_HTQaware_rebase =\n      haven::labelled(futurestudy_HTQaware_rebase\n                      , labels = futurestudy_HTQ_labels\n      ),\n    #relabelling futurestudy_plan_level45 column with futurestudy_plan_labels\n    futurestudy_plan_level45 =\n      haven::labelled(futurestudy_plan_level45\n                      , labels = futurestudy_plan_labels\n      ) )\n\n#check labels after assigning labels to codes- you should see the labels under the numbers in the console\n\npplv_data$futurestudy_consider_bachelors\n\nAfter running the code above, you should see the labels with the corresponding codes in the console underneath the numbers.\nIf you have multiple variables with the same response scale, you can add labels using a combination of dplyr::mutate() and dplyr::across():\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# First you define a list that contains the response list and codes\nlabels &lt;- c(\"Don't know\" = -1,\n            \"Very likely\" = 1,\n            \"Fairly likely\" = 2,\n            \"Not very likely\" = 3,\n            \"Not at all likely\" = 4,\n            \"I've never heard of this type of qualification\" = 5)\n\n# Then you define a list with the names of the variables you want to add labels for\nvariables &lt;- c(\"futurestudy_consider_bachelors\",\n               \"futurestudy_consider_traineeship\",\n               \"futurestudy_consider_appren\",\n               \"futurestudy_consider_degappren\",\n               \"futurestudy_consider_level45\")\n\n\n# Then you use mutate across to label multiple variables at once, \n# To adapt this, you need to change the labels and variables above.\n\npplv_data &lt;- pplv_data %&gt;%\n  dplyr::mutate(dplyr::across(all_of(variables)\n                                        , ~ haven::labelled(., labels=labels)))\n\n\n\n2.3.3 Labelling the data automatically using a dictionary\nYou can use the custom function labelling_data() to assign labels to the codes in your data using a dictionary file. You can use this function to save you time when it comes to assigning labels to codes in data instead of assigning them manually as shown above. It allows you to keep the codes in the data while storing the labels for those codes. It will check if the dictionary has matching variables in the data. If there is a match:\n\nthe matched variables will be labelled in the data frame.\nthe matched variables will be labelled with the question text found in the data dictionary (optional – not required if you just want to apply value labels).\n\n\n\n\n\n\n\nWarning\n\n\n\nThis function will only work as intended if the requirements below are met.\nIf there is a code in the data that you want to label but it does not have a corresponding label in the dictionary, that code will remain unchanged in the result. This is demonstrated in the example below.\n\n\n\nRequirements\nThe following conditions need to be met for this function to work:\n\nThe dictionary must NOT contain any blanks in the columns that contain the codes, labels or question text (if used) that correspond to the variables of interest. Otherwise, those blanks will be used.\nDictionary codes are only numeric values.\nThe dictionary and the data file should be stored in separate files.\nThe variable names in the dictionary and the data frame match. Any unmatched variable will be skipped.\nEach code and label MUST have a corresponding variable as shown in the example below.\nThe dictionary is formatted properly (more on that in the loading in separate data dictionaries section).\n\nYou can standardise NA values by doing the following:\n\nClean the dictionary file to make sure that the value code and value label columns have no non-standard blank values (e.g. “n/a”). This is shown in the code below.\n\nBelow is an example of what a dictionary should look like:\nNotice each code and label has a corresponding variable. This is most essential part.\n\n\n\nLoading in separate data dictionaries\nFirst, we will read in the data and the dictionary. We will format the dictionary to make sure it is in the required format.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#read in data\npplv_data &lt;- data.table::fread(\"data/pplv_dummy_data.csv\")\n\n#read in dictionary file\npplv_data_dic &lt;- data.table::fread(\"data/2024-02_PPLV_PupilsLearners_NS_Dic.csv\")\n\n#format the dictionary \npplv_data_dic &lt;- pplv_data_dic %&gt;%\n  #format NA values in Value_code and value_label\n  dplyr::mutate(Value_code=if_else(Value_code == \"n/a\", NA_character_,Value_code) ) %&gt;%\n  dplyr::mutate(Value_label=if_else(Value_label == \"n/a\", NA_character_, Value_label)) %&gt;%\n  #ensure that the numeric codes are in the correct format\n  dplyr::mutate(Value_coden=as.numeric(as.character(Value_code)) )\n\n\n\n\n\n\n\nNAs Introduced by Coercion\n\n\n\nYou may see the warning saying something along the line of NAs introduced by coercion when you run the code above.\nThis warning message occurs when you use as.numeric() to convert a vector in R to a numeric vector and there happen to be non-numerical values in the original vector.\nTo be clear, you don’t need to do anything to “fix” this warning message. R is simply alerting you to the fact that some values in the original vector were converted to NAs because they couldn’t be converted to numeric values.\n\n\nThen we will check the values in the data to see what they look like before labelling the data.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#view the un-labelled data set\nView(pplv_data)\n\nYou can also use the View() function to look at what the data looks like before labelling. After running the code above, you should see the pplv data with un-labelled columns that have numerical codes as shown in the image below.\n\n\n\n\n\n\nSpelling View()\n\n\n\nMake sure to spell View() with a capital V whenever you use this function. Otherwise, you will get an error.\n\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#check data before assigning labels to codes- you should just see numbers and NAs\n\npplv_data$futurestudy_consider_bachelors\n\nAfter running the code above, you should see numbers and NAs for futurestudy_consider_bachelors without any labels as shown in the image below.\n\nThen we will use labelling_data() to assign labels to the codes in pplv data using the dictionary for that data set.\nThe column names in the dictionary that the code loops through in the example below are :\n\nValue_code - the column where the codes are stored in the dictionary.\nValue_label - the column where the labels are stored in the dictionary.\nVariable - the column where the names of the variables are stored in the dictionary.\nQuestion text - the column that contains question text that you can use for labelling columns. This column is optional.\n\nYour column names might not match the ones in this example. If this is the case, you should make sure to use the column names that correspond to the dictionary you are using. The values assigned to each part of the function should match your dictionary file. For example, if your value label column is named value_label, you would assign it as dict_label = \"value_label.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#assign labels the codes\n\npplv_data&lt;- labelling_data(\n  #name of the data to be labelled\n  data= pplv_data,\n  #name of the dictionary\n  dict =pplv_data_dic,\n  #the column where the names of the variables are stored in the dictionary.\n  dict_variable= \"Variable\",\n  #the column where the labels are stored in the dictionary.\n  dict_label= \"Value_label\",\n  # the column where the codes are stored in the dictionary.\n  dict_code= \"Value_code\",\n  # the column where questions are stored in the dictionary.\n  dict_question = \"Question text\")\n\nYou will see when running the code above, a warning message will appear listing the names of those variables and the codes that do not match as shown in the image below. NAs will be included in these. This will happen if the codes in your data do not match exactly with the codes in the dictionary. Those codes without a label will remain unchanged in the data.\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# test that the assigning labels to codes worked - you should see the labels under the numbers in the console\npplv_data$futurestudy_consider_bachelors\n\nAfter assigning labels to codes, run the code chunk above, you should see labels under the codes as shown in the image below. You will also see that the column has also been labelled with the question from the dictionary. This is an optional functionality of labelling_data().\n\nNow you can use haven::as_factor() to label the data if you want to display the labels in the data instead of the codes.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n #create a labelled version of the data\n\npplv_data_labelled &lt;- haven::as_factor(pplv_data)\n\nYou can also use the View() function to look at what the data looks like after labelling. After running the code below, you should see the pplv_data_labelled with labelled columns that have labels instead of numerical codes except for the codes that had no matches (5 in futurestudy_consider_bachelors and NAs in all the columns) as shown in the image below.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n #View the labelled data set\n # you should see labels instead of codes and that the columns have been labelled with text\nView(pplv_data_labelled)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading and labelling data</span>"
    ]
  },
  {
    "objectID": "formatting_filtering.html",
    "href": "formatting_filtering.html",
    "title": "3  Formatting and filtering",
    "section": "",
    "text": "3.1 Load packages, functions and read in data\nYou can use 03_formatting_filtering.R as a reference for this section.\nThis section will cover different ways of formatting, re-coding and filtering survey data.\nYou need to make sure you load the required packages and custom functions if you haven’t already. You also need to read in the required data for this script.\nEven if you read in the data before, we recommend you re-run this code so that you’re working with a data set without any alterations you may have done to it in previous scripts.\n# load packages and custom functions --------------------------------------\n\n\nsource(\"00_packages_and_functions.R\")\n\n\n# read in data for this script --------------------------------------------\n\n\nTELSdata &lt;- haven::read_spss(\"data/TELS_DUMMY_DATA.sav\"\n                             #ensures that user-defined missing values in the spss file\n                             # are read into R as NA\n                             # If you want to bring in these codes into analysis (e.g. you might be interested\n                             # in the share who refused) change to TRUE.\n                             , user_na=FALSE)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Formatting and filtering</span>"
    ]
  },
  {
    "objectID": "formatting_filtering.html#load-packages-functions-and-read-in-data",
    "href": "formatting_filtering.html#load-packages-functions-and-read-in-data",
    "title": "3  Formatting and filtering",
    "section": "",
    "text": "Run the code\n\n\n\nRun the code below in the corresponding script in your project.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Formatting and filtering</span>"
    ]
  },
  {
    "objectID": "formatting_filtering.html#re-coding",
    "href": "formatting_filtering.html#re-coding",
    "title": "3  Formatting and filtering",
    "section": "3.2 Re-coding",
    "text": "3.2 Re-coding\nBefore re-coding, it’s useful to run a frequency of the variable to see how results look and what the value code / labels are. We will be creating those frequency tables using survey_freq_table().\n\n\n\n\n\n\nNote\n\n\n\nWe will be using the function survey_freq_table() and survey_crosstab() to create frequency tables and crosstabs in this section to check our work. We will expand on these later on but for now, we will use them to create tables as shown in the code examples.\n\n\nIn this example, I want to re-code the Grade variable so that I have a ‘pass’ and ‘fail’. To do this we will need to:\n\ncheck what the data looks like before re-coding\nre-code 1-4 (pass) into 1, and 5 (fail) into 0.\nadd value labels\nre-check the data\n\nAll of these steps are broken down in different code snippets below.\nFirst, we will check the values in the data before re-coding as shown below.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#  run a frequency of the variable to see how results look and what\n# the value code / labels are.\n\nsurvey_freq_table(TELSdata,\n                  variable=\"Grade\",\n                  weight=\"TechEd_W3_cross\")\n\nYou should see in the console a frequency table with all the categories for grade as shown in the image below.\n\nThen we re-code by using dplyr::case_when() and dplyr::mutate(). An example of how to do this for Grade is shown below.\n\n#example of recoding using case_when\nTELSdata &lt;- TELSdata %&gt;%\n  mutate(Grade_recode=case_when(\n    # if 1 or 2 are in Grade, code as 1 in Grade_recode\n    Grade %in% c(1,2) ~ 1,\n    # if 3 or 4 are in Grade, code as 2 in Grade_recode\n    Grade %in% c(3,4) ~ 2))\n\n\n\n# Adds value labels\n\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(Grade_recode= haven::labelled(Grade_recode\n                                              #label based on code\n                                              , labels= c(\"Pass/merit\"=1,\n                                                          \"Distinction/starred distinction\"=2)))\n\n# Check if the recode and labelling looks about right.\n\nsurvey_freq_table(TELSdata,\n                  variable=\"Grade_recode\",\n                  weight=\"TechEd_W3_cross\")\n\n# Cross tab with original variable to confirm.\n\nsurvey_crosstab(TELSdata,\n                   x=\"Grade\",\n                   y=\"Grade_recode\",\n                   weight=\"TechEd_W3_cross\")\n\nYou should see a frequency table and a crosstab in the console as shown in the image below. The crosstab will show the old categories and the new ones for cross checking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Formatting and filtering</span>"
    ]
  },
  {
    "objectID": "formatting_filtering.html#deriving-variables",
    "href": "formatting_filtering.html#deriving-variables",
    "title": "3  Formatting and filtering",
    "section": "3.3 Deriving variables",
    "text": "3.3 Deriving variables\nYou can also use the methods mentioned above to create a derived variable using multiple input variables. To do this, you should:\n\ncheck the values you want to re-code\ncreate your derived variable based on those values.\n\nIn this example, we will create a derived variable called Ful_recomm based on CurrentSit and Recommend. We will do this by combining those who are fulfilled with their current situation and would recommend their course.\n\n\n\n\n\n\nNote\n\n\n\nThe 'TRUE' command is like 'ELSE' in SPSS. It’s a catch all code for all values that don’t meet a specified condition. NAs are not included by default.\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#check the values in currentsit\nunique(TELSdata$CurrentSit)\n\n#check the values in recommend\nunique(TELSdata$Recommend)\n\n#create derived variable \nTELSdata &lt;- TELSdata %&gt;%\n   dplyr::mutate(Ful_recomm=case_when(\n    CurrentSit %in% c(1,2) & Recommend %in% c(1,2) ~ 1,\n    CurrentSit %in% c(1,2) & Recommend %in% c(4,5) ~ 2,\n    CurrentSit %in% c(4,5) & Recommend %in% c(1,2) ~ 3,\n    TRUE ~ 4))\n\n#label the derived variable column \nTELSdata &lt;- TELSdata %&gt;% \n  dplyr::mutate(Ful_recomm= haven::labelled(Ful_recomm,\n                                            labels=c(\"Fulfilled and recommend\" = 1,\n                                                     \"Fulfilled but don't recommend\" = 2,\n                                                     \"Not Fulfilled and don't recommend\" = 3,\n                                                     \"Other\" = 4\n                                            ) ))\n# Check the recode.\n\nsurvey_freq_table(TELSdata,\n                  variable=\"Ful_recomm\",\n                  weight=\"TechEd_W3_cross\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Formatting and filtering</span>"
    ]
  },
  {
    "objectID": "formatting_filtering.html#missing-values",
    "href": "formatting_filtering.html#missing-values",
    "title": "3  Formatting and filtering",
    "section": "3.4 Missing values",
    "text": "3.4 Missing values\nSometimes you may want to code some values as missing to run analysis like correlations or regressions. For example, if you want to correlate two questions with a Likert scale, you would want to code any don’t knows or refusals as missing (if not already defined).\nIn this example, we will change the 5 in the InstitutionAwareness column to NA as its label is “Don’t Know”. We will do this by:\n\nchecking the labels in the column of interest\nmodifying the label in that column using dplyr::na_if()\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#check the values in InstitutionAwareness\nTELSdata %&gt;% \n  dplyr::distinct(InstitutionAwareness)\n\n#combine mutate and na_if to re-code 5 to NA\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(InstitutionAwareness_recode = dplyr::na_if(InstitutionAwareness, 5))\n\n#check the values in InstitutionAwareness after the change\n# you should not be able to see 5 anymore as it has been grouped with NA\nTELSdata %&gt;% \n  dplyr::distinct(InstitutionAwareness_recode)\n\nYou should see a list of codes in the InstitutionAwareness column in the console output and then after you run dplyr::na_if(), you should see that the 5 is no longer there in InstitutionAwareness_recode. An example of this is shown in the image below.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Formatting and filtering</span>"
    ]
  },
  {
    "objectID": "formatting_filtering.html#filtering",
    "href": "formatting_filtering.html#filtering",
    "title": "3  Formatting and filtering",
    "section": "3.5 Filtering",
    "text": "3.5 Filtering\nFiltering works differently in R. You need to create a new data frame based on a filter condition. When you run it, you will see it in your R Studio Environment.\nThis means you can refer to different versions of the same data set depending on the group you’re interested in. We will create a filtered data set for males below, but first we will check the count for FF_Sex in the data.\n\n\n\n\n\n\nTip\n\n\n\nEnsure that you name each data frame clearly and sensibly. Your R environment can quickly become messy and hard to follow what is what.\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#check values for sex \n\nTELSdata %&gt;% \n  dplyr::count(FF_Sex)\n\n# create a new dataset of males - you should see that the count is the same as the\n# the total for males in the output above\n\nTELSdata_male &lt;- TELSdata %&gt;%\n  filter(FF_Sex==2)\n\n# This runs a frequency table on the new filtered dataset we just created.\n\nsurvey_freq_table(TELSdata_male,\n                variable=\"WrkStud\",\n                weight=\"TechEd_W3_cross\")\n\nYou should see the count in the console, after you run this code. You should also see that the number of obs for TELSdata_male in the environment matches the count for males in the original data set. An image of what you should see in the environment is shown below.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Formatting and filtering</span>"
    ]
  },
  {
    "objectID": "freq_tables.html",
    "href": "freq_tables.html",
    "title": "4  Frequency tables",
    "section": "",
    "text": "4.1 Load packages, functions and read in data\nYou can use 04_frequency_tables.R as a reference for this section.\nThis section will cover how to use the custom function survey_freq_table(). This function can be used to create weighted and unweighted frequency tables. It can also be used for one variable or over multiple variables.\nYou need to make sure you load the required packages and custom functions if you haven’t already. You also need to read in the required data for this script.\nEven if you read in the data before, we recommend you re-run this code so that you’re working with a data set without any alterations you may have done to it in previous scripts.\n# load packages and custom functions --------------------------------------\n\n\nsource(\"00_packages_and_functions.R\")\n\n\n# read in data for this script --------------------------------------------\n\n\nTELSdata &lt;- haven::read_spss(\"data/TELS_DUMMY_DATA.sav\"\n                             #ensures that user-defined missing values in the spss file\n                             # are read into R as NA\n                             # If you want to bring in these codes into analysis (e.g. you might be interested\n                             # in the share who refused) change to TRUE.\n                             , user_na=FALSE)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequency tables</span>"
    ]
  },
  {
    "objectID": "freq_tables.html#load-packages-functions-and-read-in-data",
    "href": "freq_tables.html#load-packages-functions-and-read-in-data",
    "title": "4  Frequency tables",
    "section": "",
    "text": "Run the code\n\n\n\nRun the code below in the corresponding script in your project.\n\n\n\n\n\n\n\n\n\nOutputs\n\n\n\nAll the functions on this page have three modes of output:\n\nNULL: save table to the environment - single variables only\n\"print\": results will be printed in the console - single and multiple variables\n\"download\": save as an Excel sheet in the working directory - single and multiple variables\n\nNOTE: if you use a function to download multiple tables without using table_name to specify different table names for your outputs then your table will be overwritten every time you download it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequency tables</span>"
    ]
  },
  {
    "objectID": "freq_tables.html#survey_freq_table",
    "href": "freq_tables.html#survey_freq_table",
    "title": "4  Frequency tables",
    "section": "4.2 survey_freq_table()",
    "text": "4.2 survey_freq_table()\nThis section will cover how to use the custom function survey_freq_table(). The frequency tables produced by this function will contain the following columns:\n\nA column with the name of the variable you want the frequency table for\n\nThe column will list the categories available in your variable\n\nFreq - Frequency of each category in your variable\nProp - The proportion of each category in your variable\n\nFor example, if we have a data set with males and females as categories in a variable called sex, frequency tells us the number of times they appear in the data. Proportion tells us the fraction of the total that each category represents. Therefore, if there are 60 males and 40 females in the data, then those numbers will be reflected in the table generated by survey_freq_table() in the frequency column. The proportion of males would be 0.6, and the proportion of females would be 0.4.\n\n4.2.1 Weighted frequency tables\n\nSingle variable\nYou can use survey_freq_table() for a single variable (column). This means that the function will create a table based on that variable alone. The table generated this way can be saved to the environment, printed or downloaded as an Excel document.\nWe will create a weighted frequency table for FF_Sex using survey_freq_table() below.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#save to environment - single var only\n\nsex_freq_table &lt;- survey_freq_table(data=TELSdata,\n                                    variables = \"FF_Sex\",\n                                    weight=\"TechEd_W3_cross\")\n\n#print\nsurvey_freq_table(data=TELSdata,\n                  variables = \"FF_Sex\",\n                  weight=\"TechEd_W3_cross\",\n                  output = \"print\")\n\n#download\n\nsurvey_freq_table(data=TELSdata,\n                  variables = \"FF_Sex\",\n                  weight=\"TechEd_W3_cross\",\n                  output = \"download\")\n\nAfter you run the code above, you will see output in the console similar to the image below. This shows a frequency table. You will find that same frequency table downloaded in your R project folder and saved in your environment under sex_freq_table.\n\n\n\nMultiple variables\nYou can use survey_freq_table() for multiple variables at once. This means that the function will iterate itself over multiple columns you specify within the data so that you do not need to write the function multiple times for multiple columns in the same data. The only thing you need to consider is that the tables generated this way can only be printed or downloaded as an Excel document.\nWe will create a weighted frequency table for FF_Sex, DV_Ethnicity, SEN_DV, WrkStud, and NextStepStudyGeneralField using survey_freq_table() below.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# List of variables\nvars &lt;- c(\"FF_Sex\", \"DV_Ethnicity\", \"SEN_DV\", \"WrkStud\", \"NextStepStudyGeneralField\")\n\n#print\nsurvey_freq_table(data=TELSdata,\n                  variables = vars,\n                  weight=\"TechEd_W3_cross\",\n                  output = \"print\")\n\n\n\n#download\n\nsurvey_freq_table(data=TELSdata,\n                  variables = vars,\n                  weight=\"TechEd_W3_cross\",\n                  output = \"download\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequency tables</span>"
    ]
  },
  {
    "objectID": "freq_tables.html#unweighted-data",
    "href": "freq_tables.html#unweighted-data",
    "title": "4  Frequency tables",
    "section": "4.3 unweighted data",
    "text": "4.3 unweighted data\nTo create unweighted frequency tables for your data, omit the specification of the weight parameter in the survey_freq_table() function.\n\n\n\n\n\n\nOptional - Find out how we create unweighted frequency tables\n\n\n\n\n\nWhen you don’t specify the weight parameter, it automatically becomes assigned NULL. This prompts survey_freq_table() to create an artificial weight set to 1 (i.e. every case is treated equally) and use that instead.\n\n\n\nThe example below shows how to create unweighted frequency tables using survey_freq_table() for WrkStud.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# This creates an unweighted frequency table for the variable 'WrkStud\nsurvey_freq_table(data=TELSdata,\n                  variables = \"WrkStud\",\n                  output = \"print\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequency tables</span>"
    ]
  },
  {
    "objectID": "freq_tables.html#combine-the-learning",
    "href": "freq_tables.html#combine-the-learning",
    "title": "4  Frequency tables",
    "section": "4.4 Combine the learning",
    "text": "4.4 Combine the learning\nIt’s important to bear in mind that the code in this guide can be combined to make your analysis more suitable to your needs.\nIn the example below we will create a frequency table but for males only. We will combine our code for filtering and creating frequency tables using survey_freq_table() for this.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# create a new dataset of males\n\nTELSdata_male &lt;- TELSdata %&gt;%\n  filter(FF_Sex==2)\n\n#freq table for the filtered dataset we just created.\n\nsurvey_freq_table(TELSdata_male,\n                 variables=\"WrkStud\",\n                 weight=\"TechEd_W3_cross\",\n                 output = \"print\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Frequency tables</span>"
    ]
  },
  {
    "objectID": "crosstabs.html",
    "href": "crosstabs.html",
    "title": "5  Cross tabulations",
    "section": "",
    "text": "5.1 x, y and z\nYou can use 05_crosstabs.R as a reference for this section.\nYou can cross tabulate two or three variables using the custom function survey_crosstab(). It can be used to create:\nThe options listed above are not separate outputs. For example, you can create a weighted two-way cross tabulation for a single variable, or you can create an unweighted three-way cross tabulation for multiple variables, etc.\nYou will use the parameters x, y and z in this section for the survey_crosstab() function. Here is a summary of what those parameters mean:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cross tabulations</span>"
    ]
  },
  {
    "objectID": "crosstabs.html#x-y-and-z",
    "href": "crosstabs.html#x-y-and-z",
    "title": "5  Cross tabulations",
    "section": "",
    "text": "x: This is the independent variable. It’s the main variable you want to analyze.\ny: This is the dependent variable. It’s the variable you want to see in relation to the independent variable.\nz: This is the control variable. It adds a third dimension to your crosstab, allowing you to see how the relationship between x and y changes across different levels of z.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cross tabulations</span>"
    ]
  },
  {
    "objectID": "crosstabs.html#load-packages-functions-and-read-in-data",
    "href": "crosstabs.html#load-packages-functions-and-read-in-data",
    "title": "5  Cross tabulations",
    "section": "5.2 Load packages, functions and read in data",
    "text": "5.2 Load packages, functions and read in data\nYou need to make sure you load the required packages and custom functions if you haven’t already. You also need to read in the required data for this script.\nEven if you read in the data before, we recommend you re-run this code so that you’re working with a data set without any alterations you may have done to it in previous scripts.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project.\n\n\n\n# load packages and custom functions --------------------------------------\n\n\nsource(\"00_packages_and_functions.R\")\n\n\n# read in data for this script --------------------------------------------\n\n\nTELSdata &lt;- haven::read_spss(\"data/TELS_DUMMY_DATA.sav\"\n                             #ensures that user-defined missing values in the spss file\n                             # are read into R as NA\n                             # If you want to bring in these codes into analysis (e.g. you might be interested\n                             # in the share who refused) change to TRUE.\n                             , user_na=FALSE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cross tabulations</span>"
    ]
  },
  {
    "objectID": "crosstabs.html#survey_crosstab",
    "href": "crosstabs.html#survey_crosstab",
    "title": "5  Cross tabulations",
    "section": "5.3 survey_crosstab()",
    "text": "5.3 survey_crosstab()\nThis section will cover how to use the custom function survey_crosstab(). The cross tabulations produced by this function will contain the following columns by default:\n\nA column for the categories in the x variable\nA column for the categories in the z variable - for three-way cross tabulations\nA column for the categories in the y variable for two-way crosstabs. For three-way crosstabs, these values will be across a row instead so that the data is in wide format.\nFreq - contains frequencies\nn - contains totals\nProp - contains proportions\n\n\n5.3.1 Single variables\nYou can use survey_crosstab() for a single variable (column). This means that the function will create a table based on that variable alone. You can do this for two-way or three-way crosstabs. The table generated this way can be saved to the environment, printed or downloaded as an Excel document.\n\nTwo-way cross tabs\n\n\n\n\n\n\nData format\n\n\n\nThe survey_crosstab() function will create tables in a long format for two-way crosstabs as they are better for data analysis. If you want to convert the data to wide format, you can use the documentation for the tidyr::pivot_wider() function to help you do that.\n\n\nWe will create a cross tabulation for FF_Sex and WrkStud below.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#save to environment\n\nsex_work_stud_cross &lt;- survey_crosstab(\n  #outline data\n  data = TELSdata,\n  #outline independent variable of crosstab\n  x = \"FF_Sex\",\n  #outline dependent variable of crosstab\n  y = \"WrkStud\",\n  #outline weight variable\n  weight = \"TechEd_W3_cross\"\n)\n\n\n#print\nsurvey_crosstab(\n  #outline data\n  data = TELSdata,\n  #outline independent variable of crosstab\n  x = \"FF_Sex\",\n  #outline dependent variable of crosstab\n  y = \"WrkStud\",\n  #outline weight variable\n  weight = \"TechEd_W3_cross\",\n  #outline the output desired\n  output = \"print\"\n)\n\n#download\nsurvey_crosstab(\n  #outline data\n  data = TELSdata,\n  #outline independent variable of crosstab\n  x = \"FF_Sex\",\n  #outline dependent variable of crosstab\n  y = \"WrkStud\",\n  #outline weight variable\n  weight = \"TechEd_W3_cross\",\n  #outline the output desired\n  output = \"download\"\n)\n\nYou should see an output similar to the image below in the console, after you run the code above. You will also see a version of that saved in the environment under sex_work_stud_cross and an XLSX file called survey_crosstab in your R project folder with the same output. The output shows a cross tabulation between FF_Sex and WrkStud variables. It shows the frequencies, totals and proportions. The totals column (n) only has two numbers as they are the totals for each variable, FF_Sex and WrkStud.\n\n\n\nThree-way cross tabs\n\n\n\n\n\n\nData format\n\n\n\nThe survey_crosstab() function will create tables in wide format for three-way crosstabs in order to make it easier to read the data. If you want to convert the data to long format, you can use the documentation for the tidyr::pivot_longer() function to help you do that.\n\n\nWe will create a cross tabulation between FF_Sex, WrkStud and DV_Ethnicity below.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# three way cross tabs - FF_Sex, WrkStud and DV_Ethnicity\n\n#print\nsurvey_crosstab(\n  #outline data\n  data = TELSdata,\n  #outline independent variable of crosstab\n  x = \"FF_Sex\",\n  #outline control variable\n  z= \"DV_Ethnicity\",\n  #outline dependent  variable of crosstab\n  y = \"WrkStud\",\n  #outline weight variable\n  weight = \"TechEd_W3_cross\",\n  #outline the output desired\n  output = \"print\"\n)\n\nYou should see an output similar to the image below in the console, after you run the code above. The output shows a cross tabulation between the FF_Sex, WrkStud and DV_Ethnicity variables. It shows the frequencies, totals and proportions. The totals column (n) only has two numbers as it is the totals for each variable, FF_Sex, WrkStudand, and DV_Ethnicity. The Freq column shows the frequency of the intersection between each of the listed categories under FF_Sex and DV_Ethnicity. The Prop column shows the proportion of those frequencies.\n\n\n\n\n5.3.2 Multiple variables\nYou can use survey_crosstab() for multiple variables at once. This means that the function will iterate itself over multiple columns you specify within the data so that you do not need to write the function multiple times for multiple columns in the same data. You can do this for two-way or three-way crosstabs. The only thing you need to consider is that the tables generated this way can only be printed or downloaded as an Excel document.\n\nTwo-way cross tabs\nWe will create cross tabulations for FF_Sex and loop over the following as our dependent variables: WrkStud, DV_Ethnicity, SEN_DV, and NextStepStudyGeneralField.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#multiple variables\n\n# List of variables\nvars &lt;- c(\"WrkStud\",\"DV_Ethnicity\", \"SEN_DV\",  \"NextStepStudyGeneralField\")\n#create two way cross tabulations of FF_sex and the listed vars above\n\n#print\nsurvey_crosstab(\n  #outline data\n  data = TELSdata,\n  #outline independent variable of crosstab\n  x = \"FF_Sex\",\n  #outline dependent variable of crosstab\n  y =vars,\n  #outline weight variable\n  weight = \"TechEd_W3_cross\",\n  #outline the output desired\n  output = \"print\"\n)\n\n\n\nThree-way cross tabs\nWe will create cross tabulations for FF_Sex and WrkStud and loop over the following as our dependent variables: DV_Ethnicity, SEN_DV, and NextStepStudyGeneralField.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# List of variables\nvars &lt;- c(\"DV_Ethnicity\", \"SEN_DV\",  \"NextStepStudyGeneralField\")\n\n#create three way cross tabulations of FF_sex and the listed vars above\n#print\nsurvey_crosstab(\n  #outline data\n  data = TELSdata,\n  #outline independent variable of crosstab\n  x = \"FF_Sex\",\n  #outline control variable of crosstab\n  z= \"WrkStud\",\n  #outline dependent variable of crosstab\n  y = vars,\n  #outline weight variable\n  weight = \"TechEd_W3_cross\",\n  #outline the output desired\n  output = \"print\"\n)\n\n#download\nsurvey_crosstab(\n  #outline data\n  data = TELSdata,\n  #outline independent variable of crosstab\n  x = \"FF_Sex\",\n  #outline control variable of crosstab\n  z=\"WrkStud\",\n  #outline dependent variable of crosstab\n  y = vars,\n  #outline weight variable\n  weight = \"TechEd_W3_cross\",\n  #outline the output desired\n  output = \"download\"\n)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cross tabulations</span>"
    ]
  },
  {
    "objectID": "crosstabs.html#unweighted-data",
    "href": "crosstabs.html#unweighted-data",
    "title": "5  Cross tabulations",
    "section": "5.4 unweighted data",
    "text": "5.4 unweighted data\nTo create unweighted cross tabulations for your data, omit the specification of the weight parameter in the survey_crosstab() function.\n\n\n\n\n\n\nOptional - Find out how we create unweighted cross tabulations\n\n\n\n\n\nWhen you don’t specify the weight parameter, it automatically becomes assigned NULL. This prompts survey_crosstab() to create an artificial weight set to 1 (i.e. every case is treated equally) and use that instead.\n\n\n\nThe example below shows how to create unweighted cross tabulations using survey_crosstab().\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# unweighted three way cross tabs - FF_Sex, WrkStud and DV_Ethnicity\n\n#print\nsurvey_crosstab(\n  #outline data\n  data = TELSdata,\n  #outline independent variable of crosstab\n  x = \"FF_Sex\",\n  #outline control variable of crosstab\n  z= \"DV_Ethnicity\",\n  #outline dependent variable of crosstab\n  y = \"WrkStud\",\n  #outline the output desired\n  output = \"print\"\n)\n\n\n# unweighted three way cross tabs - FF_Sex, WrkStud and multiple variables\n\nsurvey_crosstab(\n  #outline data\n  data = TELSdata,\n  #outline independent variable of crosstab\n  x = \"FF_Sex\",\n  #outline control variable of crosstab\n  z= \"WrkStud\",\n  #outline dependent variable of crosstab\n  y = vars,\n  #outline the output desired\n  output = \"print\"\n)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cross tabulations</span>"
    ]
  },
  {
    "objectID": "crosstabs.html#combine-the-learning",
    "href": "crosstabs.html#combine-the-learning",
    "title": "5  Cross tabulations",
    "section": "5.5 Combine the learning",
    "text": "5.5 Combine the learning\nIt’s important to bear in mind that the code in this guide can be combined to make your analysis more suitable to your needs.\nIn the example below we will create a cross tabulation but with re-coded values for Grade. We will combine our code for re-coding and creating crosstabs using survey_crosstab() for this.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#create a crosstab of recoded grade values and sex\n\n#example of recoding using case_when\nTELSdata &lt;- TELSdata %&gt;%\n  mutate(Grade_recode=case_when(\n    Grade %in% c(1,2,3,4) ~ 1,\n    Grade == 5 ~ 0))\n\n\n\n# Adds value labels\n\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(Grade_recode= haven::labelled(Grade_recode\n                                              , labels= c(\"Fail\"=0, \"Pass\"=1)))\n#create the crosstab\nsurvey_crosstab(TELSdata,\n                x=\"Grade_recode\",\n                y=\"FF_Sex\",\n                output=\"print\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cross tabulations</span>"
    ]
  },
  {
    "objectID": "summary_tables_and_avgs.html",
    "href": "summary_tables_and_avgs.html",
    "title": "6  Summary tables and weighted averages",
    "section": "",
    "text": "6.1 Load packages, functions and read in data\nThis section will cover how to create weighted and unweighted summary tables. It will also cover how to calculate weighted averages.\nYou can use 06_summary_tables_and_avgs.R as a reference for this section.\nYou need to make sure you load the required packages and custom functions if you haven’t already. You also need to read in the required data for this script.\nEven if you read in the data before, we recommend you re-run this code so that you’re working with a data set without any alterations you may have done to it in previous scripts.\n# load packages and custom functions --------------------------------------\n\n\nsource(\"00_packages_and_functions.R\")\n\n\n# read in data for this script --------------------------------------------\n\n\nTELSdata &lt;- haven::read_spss(\"data/TELS_DUMMY_DATA.sav\"\n                             #ensures that user-defined missing values in the spss file\n                             # are read into R as NA\n                             # If you want to bring in these codes into analysis (e.g. you might be interested\n                             # in the share who refused) change to TRUE.\n                             , user_na=FALSE)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary tables and weighted averages</span>"
    ]
  },
  {
    "objectID": "summary_tables_and_avgs.html#load-packages-functions-and-read-in-data",
    "href": "summary_tables_and_avgs.html#load-packages-functions-and-read-in-data",
    "title": "6  Summary tables and weighted averages",
    "section": "",
    "text": "Run the code\n\n\n\nRun the code below in the corresponding script in your project.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary tables and weighted averages</span>"
    ]
  },
  {
    "objectID": "summary_tables_and_avgs.html#summary-tables",
    "href": "summary_tables_and_avgs.html#summary-tables",
    "title": "6  Summary tables and weighted averages",
    "section": "6.2 Summary tables",
    "text": "6.2 Summary tables\nThis section will cover how to use the custom function create_summary_table() to create weighted or unweighted summary tables.\nThese tables include the following columns:\n\nthe variable name\nunweighted observations\nweighted observations - weighted tables only\nweighted mean or mean\nminimum value\nmaximum value\nunweighted missing values\nweighted missing values - weighted tables only\n\n\n\n\n\n\n\nOutputs\n\n\n\nThe function create_summary_table() has three modes of output:\n\nNULL: save table to the environment - single variables only\n\"print\": results will be printed in the console - single and multiple variables\n\"download\": save as an Excel sheet in the working directory - single and multiple variables\n\nNOTE: if you use a function to download multiple tables without using table_name to specify different table names for your outputs then your table will be overwritten every time you download it.\n\n\n\n6.2.1 Single variable\nYou can use create_summary_table() to create a summary table of one variable in a data set.\nIn the example below, we will create a weighted summary table for FF_Age2022.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# create a summary table that gives you the mean, max, min etc. for Age\n\nst_age &lt;- create_summary_table(data=TELSdata,\n                           variables =\"FF_Age2022\"\n                           , weight = \"TechEd_W3_cross\" )\n\nYou should see st_age in the Environment pane in RStudio, after running the code above. Click on it and you should be able to see a table similar to the one shown in the image below.\n\n\n\n6.2.2 Multiple variables\nYou can also use create_summary_table() to create summary tables for different variables from the same table at the same time. However, you can only print or download those tables. You cannot save them to the environment in one go.\nIn the example below, we will print and download tables for the variables FF_Sex, FF_Ethnic5, SEN_DV and FSM_DV.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#across multiple variables\n\n# List of variables to loop through\nvariables &lt;- c(\"FF_Sex\", \"FF_Ethnic5\", \"SEN_DV\", \"FSM_DV\")\n\ncreate_summary_table(data=TELSdata,\n                     variables =variables\n                     , weight = \"TechEd_W3_cross\",\n                     output = \"print\")\n\n\n\n6.2.3 unweighted data\nTo create unweighted summary tables for your data, omit the specification of the weight parameter in the create_summary_table() function. In the example below, we will recreate the summary table above, but we will make it unweighted.\n\n\n\n\n\n\nOptional - Find out how we create unweighted summary tables\n\n\n\n\n\nWhen you don’t specify the weight parameter, it automatically becomes assigned NULL. This prompts create_summary_table() to create an artificial weight set to 1 (i.e., every case is treated equally) and use that instead.\n\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# List of variables to loop through\nvariables &lt;- c(\"FF_Sex\", \"FF_Ethnic5\", \"SEN_DV\", \"FSM_DV\")\n\ncreate_summary_table(data=TELSdata,\n                     variables =variables,\n                     output = \"print\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary tables and weighted averages</span>"
    ]
  },
  {
    "objectID": "summary_tables_and_avgs.html#weighted-averages",
    "href": "summary_tables_and_avgs.html#weighted-averages",
    "title": "6  Summary tables and weighted averages",
    "section": "6.3 Weighted averages",
    "text": "6.3 Weighted averages\nYou can use the custom function weighted_avg()to get weighted means or medians. This function will only output functions in the console. You cannot save them to the environment or download them using weighted_avg().\n\n\n\n\n\n\nOutputs\n\n\n\nThe function weighted_avg() only print its output in the console.\n\n\n\n6.3.1 Mean\nIn this example, we will calculate the weighted mean for CurrentSit and we will remove NA values from the calculation.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#calculate weighted mean for currentsit\n\nweighted_avg(data = TELSdata,\n             x=\"CurrentSit\",\n             type=\"mean\",\n             weight =\"TechEd_W3_cross\",\n             na.rm=TRUE)\n\n\n\n6.3.2 Mean by group\nIn this example, we will calculate the weighted mean for CurrentSit but we will group it by FF_Sex. We will remove NA values from the calculation as well.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\nweighted_avg(data = TELSdata,\n             x=\"CurrentSit\",\n             type=\"avg by grp\",\n             weight =\"TechEd_W3_cross\",\n             by =\"FF_Sex\" ,\n             na.rm=TRUE\n)\n\nAfter you run the code above, you should be able to see output in the console that is similar to the image below.\n\n\n\n6.3.3 Median\nIn this example, we will calculate the weighted median for CurrentSit. For these calculations, you need to specify the quantile you’re interested in. The ci parameter is for calculating the confidence interval and it defaults to TRUE, but we are setting it to FALSE in this example to omit the calculation.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# calculate weighted median for currentsit\n\n\nweighted_avg(data = TELSdata,\n             x=\"CurrentSit\",\n             #outline type of average\n             type=\"median\",\n             weight =\"TechEd_W3_cross\",\n             #set it at 0.5 (50% quantile)\n             quantile=0.5,\n             ci=FALSE)\n\nIn this example, we will calculate the median for CurrentSit but for a different quantile (top 10%).\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# adjust quantile figure to calculate other quantiles or deciles, e.g. 10%\n\nweighted_avg(data = TELSdata,\n             x=\"CurrentSit\",\n             #outline type of average\n             type=\"median\",\n             weight =\"TechEd_W3_cross\",\n             #set it for 10%\n             quantile=0.1,\n             ci=FALSE)\n\n\n\n\n\n\n\nNote\n\n\n\nYou may see the following in the output in the console of the code above: attr(,\"hasci\")[1] FALSE. This indicates whether the function has computed confidence intervals for the quantiles. When hasci is FALSE, it means that confidence intervals are NOT included in the output. When hasci is TRUE, it means that confidence intervals are included in the output.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary tables and weighted averages</span>"
    ]
  },
  {
    "objectID": "summary_tables_and_avgs.html#combine-the-learning",
    "href": "summary_tables_and_avgs.html#combine-the-learning",
    "title": "6  Summary tables and weighted averages",
    "section": "6.4 Combine the learning",
    "text": "6.4 Combine the learning\nIt’s important to bear in mind that the code in this guide can be combined to make your analysis more suitable to your needs.\nIn the example below we will create a summary table but with derived variables. We will combine our code for deriving variables and create_summary_table() for this.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#create derived variable\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(Ful_recomm=case_when(\n    CurrentSit %in% c(1,2) & Recommend %in% c(1,2) ~ 1,\n    CurrentSit %in% c(1,2) & Recommend %in% c(4,5) ~ 2,\n    CurrentSit %in% c(4,5) & Recommend %in% c(1,2) ~ 3,\n    TRUE ~ 4))\n\n#label the derived variable column\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(Ful_recomm= haven::labelled(Ful_recomm,\n                                            labels=c(\"Fulfilled and recommend\" = 1,\n                                                     \"Fulfilled but don't recommend\" = 2,\n                                                     \"Not Fulfilled and don't recommend\" = 3,\n                                                     \"Other\" = 4\n                                            ) ))\n\n#create summary table for the derived variable\ncreate_summary_table(data=TELSdata,\n                     variables =\"Ful_recomm\",\n                     output = \"print\")\n\nIn the next example, we will calculate a weighted mean for the derived variable that we created above. We will still use our knowledge for deriving variables but we will combine that with the weighted_avg() function.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#calculate weighted mean for the derived variable\n\nweighted_avg(data = TELSdata,\n             x=\"Ful_recomm\",\n             type=\"mean\",\n             weight =\"TechEd_W3_cross\" )",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary tables and weighted averages</span>"
    ]
  },
  {
    "objectID": "testing_sig.html",
    "href": "testing_sig.html",
    "title": "7  Testing for significance",
    "section": "",
    "text": "7.1 Load packages, functions and read in data\nYou can use 07_sig_testing.R as a reference for this section.\nThis section provides code for different types of significance testing. For survey data, the non-parametric tests are most common.\nYou need to make sure you load the required packages and custom functions if you haven’t already. You also need to read in the required data for this script.\nEven if you read in the data before, we recommend you re-run this code so that you’re working with a data set without any alterations you may have done to it in previous scripts.\n# load packages and custom functions --------------------------------------\n\n\nsource(\"00_packages_and_functions.R\")\n\n\n# read in data for this script --------------------------------------------\n\n\nTELSdata &lt;- haven::read_spss(\"data/TELS_DUMMY_DATA.sav\"\n                             #ensures that user-defined missing values in the spss file\n                             # are read into R as NA\n                             # If you want to bring in these codes into analysis (e.g. you might be interested\n                             # in the share who refused) change to TRUE.\n                             , user_na=FALSE)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing for significance</span>"
    ]
  },
  {
    "objectID": "testing_sig.html#load-packages-functions-and-read-in-data",
    "href": "testing_sig.html#load-packages-functions-and-read-in-data",
    "title": "7  Testing for significance",
    "section": "",
    "text": "Run the code\n\n\n\nRun the code below in the corresponding script in your project.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing for significance</span>"
    ]
  },
  {
    "objectID": "testing_sig.html#non-parametric-tests",
    "href": "testing_sig.html#non-parametric-tests",
    "title": "7  Testing for significance",
    "section": "7.2 Non-parametric tests",
    "text": "7.2 Non-parametric tests\nNon-parametric tests do not assume a distribution of the data (e.g., normal), do not require knowledge about the population and are suitable for ordinal data. For these reasons, these tests are most appropriate for use in survey data.\n\n7.2.1 Chi-squared\nChi-squared tests are used to work out if there is a significant association between two independent variables. For example, if you want to determine if there is a significant association between gender (male/female) and preference for a new product (like/dislike). You can collect data from a survey where each participant is only surveyed once (unpaired). You use an unpaired chi-squared test to see if the observed frequencies of preferences differ significantly from what would be expected by chance.\nData should be categorical to run this test, so you should re-code ordinal data if you want to use chi-squared for it.\nIn the example below, we will re-code CurrentSit as it is ordinal and then do a chi-squared test on the re-coded CurrentSit variable and FSM_DV.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#re-coding currentsit so it's two categories instead of 4\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(CurrentSit_recode=dplyr::case_when(\n    CurrentSit %in% c(1,2) ~ 1,\n    CurrentSit %in% c(4,5) ~ 2))\n\n#do a chi-squared test on currentsit and fsm_dv\nchisq &lt;- sjstats::chi_squared_test(TELSdata,\n                                   select = \"CurrentSit_recode\",\n                                   by =\"FSM_DV\",\n                                   weights= \"TechEd_W3_cross\")\n\n#view results in the console\nchisq\n\nAfter running the code above, you should see in the console that the p value is not &lt;0.05 so the effect is not significant.\n\nLongitudinal data\nIf you have a longitudinal data set, you might want to test the association between variables in a pre- and post- scenario. i.e. whether a learner’s view has changed over time. In this case you would want to do a paired chi-squared test.\nHere is an example of how to do this.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\nchisq &lt;- sjstats::chi_squared_test(TELSdata,\n                                   select = \"CurrentSit_recode\",\n                                   #define groups for paired test\n                                   #note this column does not exist in the data\n                                   by =\"wave\",\n                                   weights= \"TechEd_W3_cross\",\n                                   #do a paired test\n                                   paired=TRUE)\n\n#view results in the console\nchisq\n\nYou should see in the console that the p-value is &lt;0.05. This means that there is a significant difference in CurrentSit_recode between the different waves in the dummy data. However, the effect size (shown by ϕ) is very small, indicating that the practical impact of this difference is minimal.\n\n\n\n7.2.2 Mann-Whitney U / Kruskal-Wallis\n\nMann-Whitney U\nThis test is designed for comparing ordinal data across two groups for two independent samples, e.g., comparing the customer satisfaction ratings (on a scale of 1 to 5) between two different stores. Since the ratings are ordinal and may not be normally distributed, you use the Mann-Whitney U test to compare the two independent groups.\nYou can interpret the scores as follows:\n\nSmall effect &gt;= 0.1\nMedium effect &gt;= 0.3\nLarge effect &gt;= 0.5\n\nYou should combine the interpretation of these scores with the interpretation of the p-value as well. In the example below, we will compare CurrentSit and FF_Sex and interpret both values.\n\nLevels and categories\nWhen working with SPSS data in R, the categories in grouped data like FF_Sex can be referred to as groups, levels, and factors. It is important to be familiar with these terms as you may come across them in different documentation.\n\n\n\n\n\n\nUnused levels and sjstats::mann_whitney_test()\n\n\n\nIf the variable that you’re using for the by parameter for sjstats::mann_whitney_test() has more than two levels (categories), even if those levels (categories) are unused, they need to be dropped. Otherwise, the incorrect labels will be used for the ranking scores.\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#check how many levels (categories) there are for FF_SEX in the data\n\nunique(TELSdata$FF_Sex) # there are more than 2\n\n# Use mutate to convert FF_Sex to a factor and drop unused levels (categories) for sex\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(FF_Sex = droplevels(as.factor(FF_Sex)))\n\n#carry out test \nranktest1 &lt;- sjstats::mann_whitney_test(TELSdata,\n                                        select=\"CurrentSit\",\n                                        by=\"FF_Sex\",\n                                        weights = \"TechEd_W3_cross\")\n#view results\nranktest1\n\nYou should see in the console the result show:\n\nthere is no significant difference (p&gt;0.01), and the effect is small (R=0.05).\ngroup 1: Female had a higher rank score.\n\nin our case the scale is inverted so higher number = more likely to be unfulfilled.\n\nwith a higher rank score, we would say that the female participants are more likely to be “unfulfilled” but with a low significance with a small effect, this is likely due to random variation rather than a true difference between the groups.\n\n\n\n\nKruskal-Wallis\nIf your group variable has more than two categories, use the Kruskal-Wallis. For example, if you want to compare the effectiveness of three different diets on weight loss. You measure the weight loss of participants after following each diet for a month. Since the data may not be normally distributed and you have more than two groups, you use the Kruskal-Wallis test. The first variable in the formula is the dependent variable and the second is the independent variable or groups you want to test.\nIn the example below, we’re going to compare CurrentSit and TLPathway_Str. This means we are comparing fulfillment (shown by CurrentSit) across the different types of pathways in the data (shown by TLPathway_Str).\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#create survey design object that outlines weights\nweight &lt;- survey::svydesign(\n  #- ~1 means that there is a single constant term \n  #i.e. each observation is treated as a separate\n  # sampling unit with equal weight.\n  \n  ids = ~1,\n  #outline the weight variable \n  weights = ~TechEd_W3_cross,\n  data = TELSdata)\n\n\nkruskalwallis &lt;- survey::svyranktest(\n  # The first variable in the formula is the dependent variable\n  # the second is the independent variable or groups you want to test.\n  formula = CurrentSit ~ TLPathway_Str,\n  #use survey design object to define weight\n  design=weight,\n  # specify test \n  test=\"KruskalWallis\")\n\n#view results in the console\nkruskalwallis\n\nAfter running this code, you should see in the console that the p-value is &gt; 0.05. This means we cannot reject the null hypothesis so there is no statistically significant difference in the distributions of FF_Age2022 across the different TLPathway_Str groups.\nAfter Kruskal-Wallis, you might want to do post-hoc comparisons to identify which category is significantly different. You would use Dunn’s test for this, but we haven’t found a version in R that can handle weights.\n\n\n\n7.2.3 Wilcoxon\nThe Wilcoxon test is like the Mann-Whitney U test but for paired samples, i.e. when you want to test a pre- and post- effect with the same people. For example, if you are testing the effect of a new drug on blood pressure, you measure the blood pressure of patients before and after taking the drug. Since the data are paired and may not be normally distributed, you use the Wilcoxon signed-rank test to compare the two related samples.\nIn our example below, we are comparing how fulfilled (this is in CurrentSit) participants feel at two different points in time.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#wilcoxon without weights\nwilcoxon &lt;- sjstats::wilcoxon_test(TELSdata,\n                                   select = \"CurrentSit\",\n                                   by = \"wave\"\n)\n\n#view results in the console\nwilcoxon\n\nAfter you run the code above, you should see in the console that the p-value is &gt; 0.05. This means we cannot reject the null hypothesis as there is no statistically significant difference in CurrentSit between the different wave variations in the dummy data. The small effect size (r = 0.04) also suggests that any difference is minimal.\nNow we are going to do the same test but with weights.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#wilcoxon with weights\n\ndesign &lt;- survey::svydesign(ids = ~1,\n                            data = TELSdata,\n                            weights = ~TechEd_W3_cross)\n\nwilcoxon_weighted &lt;- survey::svyranktest(CurrentSit ~ wave,\n                    design,\n                    test = \"wilcoxon\")\n\n#view results in the console\nwilcoxon_weighted\n\n\n\n\n\n\n\nImportant\n\n\n\nThe output of this code will say “Design-based KruskalWallis test”. However, this test is a Wilcoxon test, and this message is due to the similarities in survey design between Wilcoxon and Kruskal Wallis tests.\n\n\nAfter running the code above, you should see in the console that the p-value is &gt; 0.05. This means we cannot reject the null hypothesis as there is no statistically significant difference in the mean rank scores between CurrentSit over the different waves. The difference in mean rank score is also very small (0.045), indicating minimal variation between the groups.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing for significance</span>"
    ]
  },
  {
    "objectID": "testing_sig.html#parametric-tests",
    "href": "testing_sig.html#parametric-tests",
    "title": "7  Testing for significance",
    "section": "7.3 Parametric tests",
    "text": "7.3 Parametric tests\nParametric tests assume normally distributed data, knowledge about the population parameters and typically requires interval data. Since most of survey data is nominal/ordinal and rarely normally distributed, these tests are appropriate in the minority of cases.\n\n7.3.1 T-test\nT-tests are used to determine whether two groups are significantly different from each other. There are three main types, and the dependent variable needs to be continuous for all of them, but the independent variable differs.\n\n\n\n\n\n\nData types\n\n\n\nEnsure that you have the correct data types for all tests but ensure this for t-tests and z-tests in particular as the functions are stricter with data types. They require numeric (continuous) type data.\n\n\n\nIndependent Samples T-test:\nThe independent variable is typically binary, representing two different groups (e.g., male vs. female, treatment vs. control). For example, if you want to compare the average test scores of students from two different schools, use an independent t-test. This will determine if there is a significant difference between the means of the two independent groups.\nIn the example below, we will be comparing the average age by sex.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#check that the dependent variable (age) has a numeric data type\nclass(TELSdata$FF_Age2022)\n\n#change the data type from character to numeric (to reflect continuous values)\n\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(FF_Age2022=as.numeric(FF_Age2022))\n\n# test whether male and females are significantly different\n# from each other in age\n\nttest &lt;- sjstats::t_test(TELSdata,\n                         #define test/dependent variable\n                         #this should be continuous\n                         select = \"age\",\n                         #define the groups we want to test\n                         by = \"sex\",\n                         #give column name for the weight\n                         weights = \"TechEd_W3_cross\")\n\n#view the results\nttest\n\nAfter you run the code above, you should see in the console that there is not a statistically significant difference in the mean ages between males and females as the p-vale is &lt; 0.05. The small effect size (Cohen’s d = 0.11) also suggests that any difference in ages is minimal and not practically significant.\n\n\nPaired Samples T-test:\nThe independent variable represents two related measurements (e.g., pre-test vs. post-test scores for the same individuals). This is helpful when you have a longitudinal data set, and you want to see if there is a difference over time. For example, if you measure the performance of employees before and after a training program, you can use a paired t-test. This is because the data are paired (same employees measured twice), you use a paired t-test to compare the means of the two related groups.\nIn the example below, we will compare the income of individuals in the data in two different time points. We will add the ‘paired’ operator and split by wave as that column indicates the time points.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\nttest2 &lt;- sjstats::t_test(TELSdata,\n                          select = \"income\",\n                          #split by variable that has the groups\n                          by = \"wave\",\n                          weights = \"TechEd_W3_cross\",\n                          #set paired to true\n                          paired=TRUE)\n\n#view results in the console\nttest2\n\nAfter you run the code above, you should see in the console that there is a statistically significant difference in income between the waves as the p-value is &lt; 0.05. The effect size (Cohen’s d = -0.06) suggests that, while the difference is statistically significant, the magnitude of this difference is very small in practical terms.\n\n\nOne-Sample T-test:\nThe independent variable is a single group compared to a known value or population mean. In the example below, we will compare the income variable to its mean.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# Perform one-sample t-test\nresult &lt;- sjstats::t_test(TELSdata,\n                 select = \"income\",\n                 mu = 50052.28)\n#view results in the console\nresult\n\nAfter you run the code above, you should see in the console that there is no statistically significant difference between the mean income of our sample and the hypothetical population mean of 50,052.28. The effect size is very small (Cohen’s d = -0.04), suggesting that the difference is negligible in practical terms.\n\n\n\n7.3.2 Z-test\n\n\n\n\n\n\nData types\n\n\n\nEnsure that you have the correct data types for all tests, but ensure this for t-tests and z-tests in particular, as the functions are stricter with data types. They require numeric (continuous) type data.\n\n\nZ-tests are used to determine whether there is a significant difference between sample and population means or between the means of two samples. For example, if you are comparing the average height of a sample of adult men to the known population mean height, you would use a z-test. Since you know the population standard deviation and have a large sample size, you use a z-test to determine if the sample mean significantly differs from the population mean.\nAnother example of when it is useful is when comparing the percentage of people who are satisfied between two groups (e.g. IoT (Institution of Technology) learner vs. non-IoT (non-Institution of Technology) learner).\nWe’re going to outline weighted counts and sample sizes in the example below. e.g., 86 in 185 (46%), 528 in 948 (56%).\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to reload the packages and custom functions.\n\n\n\n# determine if two proportions in large samples are significantly different\n# using dummy data\n\n# outlining weighted count for first group\np1&lt;-86\n# outlining sample size for first group\nn1&lt;-185\n# outlining weighted count for second group\np2&lt;-528\n# outlining sample size for second group\nn2&lt;-948\n\nresult &lt;-stats::prop.test(\n  #give weighted counts for each group\n  c(p1,p2),\n  #give the sample size for each group\n  n=c(n1,n2))\n\n#print result in the console\nresult\n#extract z statistic\nzstatistic &lt;-result$statistic\n\nAfter you run the code above, you should see in the console that the p-value (0.02648) is &lt; 0.05. This means we can reject the null hypothesis and there is a statistically significant difference between the two proportions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing for significance</span>"
    ]
  },
  {
    "objectID": "testing_sig.html#combine-the-learning",
    "href": "testing_sig.html#combine-the-learning",
    "title": "7  Testing for significance",
    "section": "7.4 Combine the learning",
    "text": "7.4 Combine the learning\nIt’s important to bear in mind that the code in this guide can be combined to make your analysis more suitable to your needs.\nIn the example below, we will do a chi-squared test on CurrentSit_recode and FSM_DV after filtering the data set for respondents that have finished or are currently doing a T-level qualification.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions. Make sure to run the lines for re-coding the CurrentSit variable as well for this test.\n\n\n\n#filter the data\ntels_t_level &lt;- TELSdata %&gt;%\n  dplyr::filter(FinishTLevel %in% c(1,2))\n\n#do a chi-squared test on currentsit and fsm_dv\nchisq &lt;- sjstats::chi_squared_test(tels_t_level,\n                                   select = \"CurrentSit_recode\",\n                                   by =\"FSM_DV\",\n                                   weights= \"TechEd_W3_cross\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing for significance</span>"
    ]
  },
  {
    "objectID": "testing_corr.html",
    "href": "testing_corr.html",
    "title": "8  Testing for correlation",
    "section": "",
    "text": "8.1 Load packages, functions and read in data\nYou can use 08_correlation.R as a reference for this section.\nThis section provides code for different types of correlation testing.\nYou need to make sure you load the required packages and custom functions if you haven’t already. You also need to read in the required data for this script.\nEven if you read in the data before, we recommend you re-run this code so that you’re working with a data set without any alterations you may have done to it in previous scripts.\n# load packages and custom functions --------------------------------------\n\n\nsource(\"00_packages_and_functions.R\")\n\n\n# read in data for this script --------------------------------------------\n\n\nTELSdata &lt;- haven::read_spss(\"data/TELS_DUMMY_DATA.sav\"\n                             #ensures that user-defined missing values in the spss file\n                             # are read into R as NA\n                             # If you want to bring in these codes into analysis (e.g. you might be interested\n                             # in the share who refused) change to TRUE.\n                             , user_na=FALSE)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing for correlation</span>"
    ]
  },
  {
    "objectID": "testing_corr.html#load-packages-functions-and-read-in-data",
    "href": "testing_corr.html#load-packages-functions-and-read-in-data",
    "title": "8  Testing for correlation",
    "section": "",
    "text": "Run the code\n\n\n\nRun the code below in the corresponding script in your project.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing for correlation</span>"
    ]
  },
  {
    "objectID": "testing_corr.html#suitability-of-data",
    "href": "testing_corr.html#suitability-of-data",
    "title": "8  Testing for correlation",
    "section": "8.2 Suitability of data",
    "text": "8.2 Suitability of data\nIn this section, we will cover correlation tests. Testing for correlation is used to determine whether there is a relationship between two variables. That testing can also be used to identify relationships between two Likert scales, for example, agreeing that a T Level has allowed learners to progress to what they want to do vs. feeling prepared for their future career.\nBefore doing correlation tests, we need to check that value codes are appropriate for correlation analysis.\nCorrelation output is easier to understand when:\n\nnegative responses are coded as a negative number\npositive responses are coded as a positive number\nneutral responses are coded as 0\n\nIn the example below, we will look at checking and ensuring the suitability of the value codes in Progress and PrepareCareer.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n# Check value codes are appropriate for correlation analysis\n\nunique(TELSdata$Progress)\nunique(TELSdata$PrepareCareer)\n\n# re-code responses. negative responses = negative no\n# positive repose = positive no\n# neutral as 0\n\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(\n    #re-code progress\n    Progress_recode= dplyr::case_when(\n      Progress == 5 ~ -2,\n      Progress == 4 ~ -1,\n      Progress == 3 ~ 0,\n      Progress == 2 ~ 1,\n      Progress == 1 ~ 2),\n    #re-code preparecareer\n    PrepareCareer_recode=case_when(\n      PrepareCareer == 5 ~ -2,\n      PrepareCareer == 4 ~ -1,\n      PrepareCareer == 3 ~ 0,\n      PrepareCareer == 2 ~ 1,\n      PrepareCareer == 1 ~ 2)\n  )\n\nOnce variables are correctly coded for correlation analysis, decide what method is appropriate:\n\nSpearman’s Rho: non-parametric test and ordinal data.\nKendall’s Tau: non-parametric test and small sample sizes\nPearson’s: parametric test and continuous data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing for correlation</span>"
    ]
  },
  {
    "objectID": "testing_corr.html#spearmans-rho",
    "href": "testing_corr.html#spearmans-rho",
    "title": "8  Testing for correlation",
    "section": "8.3 Spearman’s Rho",
    "text": "8.3 Spearman’s Rho\nSpearman’s is the most appropriate correlation method for most survey data since it’s non-parametric and can work with ordinal data. For example, you could use Spearman’s if you want to study the relationship between students’ ranks in two different subjects, like Math and Science. Since the data are ordinal (ranks) and you want to assess the correlation between the two sets of ranks, you use Spearman’s rho.\nHowever, there isn’t currently an R function that can calculate Spearman’s with weighted data. The function below is based on unweighted data. This isn’t a huge issue since it’s a rank-based measure.\nBelow, we are going to run code to examine the relationship between the progress of participants (shown by Progress_recode) and how prepared they feel for their career (shown by PrepareCareer_recode).\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\ncor_test &lt;- stats::cor.test(\n  # specify the values you want to test for correlation\n  TELSdata$Progress_recode,\n  TELSdata$PrepareCareer_recode,\n  #indicate the alternative hypothesis\n  alternative = \"two.sided\",\n  # specify method\n  method =  \"spearman\",\n  #  specify confidence level\n  conf.level = 0.95)\n\nAfter you run the code above, you should see in the console there is a Rho value of 0.6748364. This shows there is a positive correlation between Progress_recode and PrepareCareer_recode. This suggests that as Progress_recode increases, PrepareCareer_recode tends to increase as well, and vice versa. The p-value is &lt; 0.05, which indicates that this correlation is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing for correlation</span>"
    ]
  },
  {
    "objectID": "testing_corr.html#kendalls-tau",
    "href": "testing_corr.html#kendalls-tau",
    "title": "8  Testing for correlation",
    "section": "8.4 Kendall’s Tau",
    "text": "8.4 Kendall’s Tau\nKendall’s is similar to Spearman’s, but it is better suited to the following:\n\nsmall sample sizes, i.e., &lt;30.\ndealing with ‘tied ranks’\n\nthis can be useful if you’re correlating two short scales (i.e., higher probability of the same combination occurring).\n\ndealing with outliers.\n\nFor example, you could use Kendall’s if you are analyzing the consistency of rankings given by two assessors in an interview. The data are ordinal, and you want a measure that is robust to ties and small sample sizes. Kendall’s is appropriate here because it handles ties better and is more robust in small samples compared to Spearman’s Rho.\nSpearman’s is more well known, but Kendall’s is growing in popularity and can be used in Spearman’s place even with large samples.\nIn the example below, we will replicate what we did for Spearman’s correlation test but using Kendall’s instead.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to create extra dummy data and reload the packages and custom functions.\n\n\n\ncor_test &lt;- stats::cor.test(\n  # specify the values you want to test for correlation\n  TELSdata$Progress_recode,\n  TELSdata$PrepareCareer_recode,\n  #indicate the alternative hypothesis\n  alternative = \"two.sided\",\n  # specify method\n  method =  \"kendall\",\n  #  specify confidence level\n  conf.level = 0.95)\n\nAfter you run the code above, you should see in the console there is a Tau value of 0.6271273. This shows there is a positive correlation between Progress_recode and PrepareCareer_recode. This suggests that as Progress_recode increases, PrepareCareer_recode tends to increase as well, and vice versa. The p-value is &lt; 0.05, which means that this correlation is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing for correlation</span>"
    ]
  },
  {
    "objectID": "testing_corr.html#pearsons",
    "href": "testing_corr.html#pearsons",
    "title": "8  Testing for correlation",
    "section": "8.5 Pearson’s",
    "text": "8.5 Pearson’s\nIt’s a way to measure the degree of a relationship between two linearly related variables. It uses the Pearson correlation coefficient test to compare the mean value of the product of the standard scores of matched pairs of observations. This yields an answer in the range of -1 to +1.\nTo use the Pearson correlation, each variable must be continuous, and the shape of the variable’s relationship must be linear. If these two assumptions are not applicable, you should use Spearman’s correlation instead.\nFor example, you could use Pearson’s if you are investigating the relationship between the number of hours studied and the scores obtained in an exam. Both variables are continuous, and you assume a linear relationship between them. Pearson’s correlation is suitable here as it measures the strength and direction of the linear relationship between the two continuous variables.\nIn the code below, we will examine the correlation between age and income.\n\n\n\n\n\n\nImportant\n\n\n\nThe variables used in the syntax below are not appropriate for Pearson’s since they are not continuous. They are only used as an example.\n\n\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions.\n\n\n\n#check that the dependent variable (age) has a numeric data type\nclass(TELSdata$FF_Age2022)\n\n#change the data type from character to numeric (to reflect continuous values)\n\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(FF_Age2022=as.numeric(FF_Age2022))\n\n#create survey design object that outlines weights\nweight &lt;- survey::svydesign(ids = ~1,\n                            weights = ~TechEd_W3_cross,\n                            data = TELSdata)\n\n\n#store the details in an object\ncorr &lt;- jtools::svycor(~age + income,\n                       design = weight,\n                       #removes NAs from the calculation\n                       na.rm=TRUE,\n                       #provide all details not just coefficient\n                       sig.stats=TRUE)\n\n#extract correlation coefficient\ncorr$cors\n#extract p value\ncorr$p.values\n#extract std error\ncorr$std.err\n\nThe corr$cors should print in the console and show that the two variables have almost no correlation, and the p-value from corr$p.values is more than 0.01 so it’s not significant.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing for correlation</span>"
    ]
  },
  {
    "objectID": "testing_corr.html#combine-the-learning",
    "href": "testing_corr.html#combine-the-learning",
    "title": "8  Testing for correlation",
    "section": "8.6 Combine the learning",
    "text": "8.6 Combine the learning\nIt’s important to bear in mind that the code in this guide can be combined to make your analysis more suitable to your needs.\nIn the example below, we will do a correlation test on Progress_recode and PrepareCareer_recode after creating a derived variable and filtering the data set.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below in the corresponding script in your project. If you are starting from this point in the guide, remember to run lines from the top of the script to read in the data and reload the packages and custom functions. Make sure to run the lines for re-coding the variables as well for this test.\n\n\n\n#create derived variable\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(Ful_recomm=case_when(\n    CurrentSit %in% c(1,2) & Recommend %in% c(1,2) ~ 1,\n    CurrentSit %in% c(1,2) & Recommend %in% c(4,5) ~ 2,\n    CurrentSit %in% c(4,5) & Recommend %in% c(1,2) ~ 3,\n    TRUE ~ 4))\n\n#label the derived variable column\nTELSdata &lt;- TELSdata %&gt;%\n  dplyr::mutate(Ful_recomm= haven::labelled(Ful_recomm,\n                                            labels=c(\"Fulfilled and recommend\" = 1,\n                                                     \"Fulfilled but don't recommend\" = 2,\n                                                     \"Not Fulfilled and don't recommend\" = 3,\n                                                     \"Other\" = 4\n                                            ) ))\n\n#create a filtered data set for fulfilled and recommend\ntels_ful_recomm &lt;- TELSdata %&gt;%\n  dplyr::filter(Ful_recomm==1)\n\n#carry out correlation test on the filtered data set to test correlation between\n# Progress_recode and PrepareCareer_recode\nstats::cor.test(\n  # specify the variables you want to test for correlation\n  tels_ful_recomm$Progress_recode,\n  tels_ful_recomm$PrepareCareer_recode,\n  #indicate the alternative hypothesis\n  alternative = \"two.sided\",\n  # specify method\n  method =  \"spearman\",\n  #  specify confidence level\n  conf.level = 0.95)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Testing for correlation</span>"
    ]
  },
  {
    "objectID": "working_directory.html",
    "href": "working_directory.html",
    "title": "9  Working directory",
    "section": "",
    "text": "9.1 What is a working directory (wd)?\nYou can use 09_working_directory.R as a reference for this section.\nIn R, the working directory is the folder on your computer where R reads and saves files by default. Think of it as the current location where R looks for data files when you load them and where it stores the results when you save them. In this section, we will look at managing the wd in your projects in an efficient way.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working directory</span>"
    ]
  },
  {
    "objectID": "working_directory.html#checking-current-working-directory",
    "href": "working_directory.html#checking-current-working-directory",
    "title": "9  Working directory",
    "section": "9.2 Checking current working directory",
    "text": "9.2 Checking current working directory\nThe default wd is usually the file path of your R project folder. You can check this by running the code below.\n\n\n\n\n\n\nRun the code\n\n\n\nRun the code below.\n\n\n\n#check working directory path \ngetwd()\n\nYou should see something similar to \"C:/Users/&lt;your_username&gt;/r_survey_guide\" in the console. This means that files are automatically read and written to this path.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working directory</span>"
    ]
  },
  {
    "objectID": "working_directory.html#changing-working-directory",
    "href": "working_directory.html#changing-working-directory",
    "title": "9  Working directory",
    "section": "9.3 Changing working directory",
    "text": "9.3 Changing working directory\nIf you have sensitive data that must be kept in certain folders, then you can change your wd to read in that data without moving it. However, we need to ensure that we revert the wd back to its original state so that no problems happen with the code later on. We will do this by:\n\nsaving the original wd as a variable.\nsaving the sensitive folder path as a variable.\nalternating between them as needed using setwd() function.\n\n\n\n\n\n\n\nRun the code\n\n\n\nAdapt the code below and run it for your own personal project when you need to change the working directory. Otherwise, this code will not work.\n\n\n\n\n\n\n\n\nUsing forward slashes\n\n\n\nWhen using folder paths in R, it is important to use forward slashes (/) instead of backslashes (\\) which are traditionally used in file paths in Windows. Otherwise the code will not work because the path will not be recognized. This applies to reading and saving files too.\n\n\n\n#store project's wd\nproject_wd &lt;- getwd()\n\n#store wd of sensitive folders you need data from\n#path must be separated with the slashes shown below\n\nsecure_folder_wd &lt;- \"E:/Publication/Data\"\n\n#set wd to sensitive folder\n\nsetwd(secure_folder_wd)\n\n#to return to original wd,\n\nsetwd(project_wd)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working directory</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "10  Working with Git",
    "section": "",
    "text": "10.1 What is Git?\nThis section is only relevant if you want to use Git (GitHub or Azure DevOps) for this guide or for your project. If you want to use Git for this project, we recommend creating your own repo with the code.\nGit is a tool that helps track changes to files in a project. It’s great for teamwork because it allows everyone to work on the same project without interfering with each other’s work. It keeps a history of all changes, so you can see what was changed and go back to previous versions if needed. This makes managing and collaborating on projects much easier and more organized. In DfE, we use GitHub for public facing projects that don’t contain sensitive code, and we use Azure DevOps for sensitive projects.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with Git</span>"
    ]
  },
  {
    "objectID": "git.html#guidance-and-workshops",
    "href": "git.html#guidance-and-workshops",
    "title": "10  Working with Git",
    "section": "10.2 Guidance and workshops",
    "text": "10.2 Guidance and workshops\nIf you are interested in learning on how to work with Git, you can use the following resources to help you learn:\n\nDfE analyst guide Git tips.\nLearn about using Azure DevOps\nAvoid revealing sensitive information when working with Git\nYou can also email statistics.development@education.gov.uk to ask about the Introduction to Git and Azure DevOps technical workshop we run in DfE.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Working with Git</span>"
    ]
  },
  {
    "objectID": "custom_functions.html",
    "href": "custom_functions.html",
    "title": "Custom functions reference",
    "section": "",
    "text": "What are custom functions and why are we using them?\nThis section will provide an overview of each of the custom functions we built for survey analysis in R. For each function, you will get a breakdown of:\nCustom functions are functions that are not part of a package. We are using them to make using functions from published packages like survey more user-friendly.",
    "crumbs": [
      "Custom functions reference"
    ]
  },
  {
    "objectID": "custom_functions.html#labelling-data",
    "href": "custom_functions.html#labelling-data",
    "title": "Custom functions reference",
    "section": "Labelling data",
    "text": "Labelling data\nThis function will assign labels to a data set using a dictionary by comparing the variables and the codes found in the data to the ones found in the dictionary. Dictionary files’ format MUST be in CSV with no gaps in the columns for variables, labels or codes.\nAny unmatched labels and variables will not be changed and will be highlighted in a warning message. To view the labels, use the function haven::as_factor().\n\nlabelling_data()\n\nlabelling_data (data,\n                dict,\n                dict_variable,\n                dict_label,\n                dict_code,\n                dict_question)\n\n\nArguments\ndata Data to be labelled.\ndict A dictionary data frame to use for labelling data.\ndict_variable The name of the column that contains the names of label variables in the dictionary. Must be in quotes.\ndict_label The name of the column that contains the labels in the dictionary. Must be in quotes.\ndict_code The name of the column that contains the code in the dictionary. Must be in quotes.\ndict_question OPTIONAL- Default is NULL. The name of the column in the dictionary that contains the question that you can use to label variables in the data. Must be in quotes.\n\n\nExample\n\n#load in the custom functions\nsource(\"00_packages_and_functions.R\")\n\n\n#create data with codes to be labelled \ndf &lt;- data.frame(\n  opinion = c(1, 2, 2, NA, 4, NA, 1),\n  group = c(1, 3, 3, 2, 2, 1, 3),\n  Sex = c(1, 1, 1, 2, 2, NA, NA)\n)\n\n#create a dictionary with variable names, codes and labels \ndict_example &lt;- data.frame(\n  variable = c(\"opinion\", \"opinion\",\"opinion\",\"opinion\",\n               \"group\",\"group\",\"group\",\"group\",\n               \"Sex\",\"Sex\"),\n  code = c(1, 2, 3, 4, 1, 2, 3, 4, 1, 2),\n  label = c(\"opinion1\",\"opinion2\",\"opinion3\",\"opinion4\",\n            \"group1\",\"group2\",\"group3\",\"group4\",\n            \"sex1\",\"sex2\"\n  )\n)\n#use labelling_data to label the df using the dictionary \n\nlabelled_df &lt;- labelling_data(\n  data = df,\n  dict = dict_example,\n  dict_variable = \"variable\",\n  dict_label = \"label\",\n  dict_code = \"code\"\n)",
    "crumbs": [
      "Custom functions reference"
    ]
  },
  {
    "objectID": "custom_functions.html#creating-frequency-tables",
    "href": "custom_functions.html#creating-frequency-tables",
    "title": "Custom functions reference",
    "section": "Creating frequency tables",
    "text": "Creating frequency tables\n\nsurvey_freq_table()\nCreate weighted and unweighted flexible frequency tables with the following columns:\n\nA column with the name of the variable you want the frequency table for\n\nThe column will list the categories available in your variable\n\nFreq - Frequency of each category in your variable\nProp - The proportion of each category in your variable\n\n\nsurvey_freq_table (data,\n                   variables,\n                   weight = NULL,\n                   output = NULL,\n                   table_name = NULL)\n\n\nArguments\ndata Data to create the frequency table from\nvariables The column name(s) for variable(s) you want the frequency table for. Must be in quotes.\nweight OPTIONAL - The name of the column that contains the weights. Must be in quotes. Default is NULL for unweighted tables.\noutput OPTIONAL FOR SINGLE VARIABLES - The type of output you want from the function. The available options are NULL, \"download\" and \"print\".\n\nNULL is the default and it allows you to save the table to the environment when you use the function for a single variable.\n\"print\" prints the frequency tables in the console - can be used for single or multiple variables.\n\"download\" downloads a xlsx with the frequency table of the specified variable and weights - can be used for single or multiple variables.\n\ntable_name OPTIONAL - A string of the name you want for the table if you use the “download” output option. If NULL, then the table downloaded will be called “frequency_table.xlsx”.\n\n\nExamples\n\n#load in the custom functions\nsource(\"00_packages_and_functions.R\")\n\n\n#create data with codes to be labelled\n\n#make the example data reproducible \nset.seed(123)\ndf &lt;- data.frame(\n  #create two variables \n  var_1= c(1,1,2,3,4,4,3,1),\n  var_2=c(1,1,2,1,2,1,2,1),\n  #create weight \n  weight= stats::runif(8, min = 0.1, max = 1))\n#create a weighted frequency table \n\nsurvey_freq_table(df,\n                 variables = \"var_1\",\n                 weight = \"weight\")\n\n#create an unweighted frequency table \n\nsurvey_freq_table(df,\n                 variables = \"var_1\")\n\n#create a weighted frequency table across multiple variables \n#print it in the console \n\nsurvey_freq_table(df,\n                 variables = c(\"var_1\",\"var_2\"),\n                 weight = \"weight\", \n                 output = \"print\")\n\n#create a weighted frequency table across multiple variables \n#download it as an Excel document \nsurvey_freq_table(df,\n                 variables = c(\"var_1\",\"var_2\"),\n                 weight = \"weight\", \n                 output = \"download\")",
    "crumbs": [
      "Custom functions reference"
    ]
  },
  {
    "objectID": "custom_functions.html#creating-cross-tabulations",
    "href": "custom_functions.html#creating-cross-tabulations",
    "title": "Custom functions reference",
    "section": "Creating cross tabulations",
    "text": "Creating cross tabulations\n\nsurvey_crosstab()\nCreate weighted and unweighted crosstabs with the following columns:\n\nA column for the categories in the x variable\nA column for the categories in the z variable - for three-way cross tabulations\nA column for the categories in the y variable\nFreq - contains frequencies\nn - contains totals\nProp - contains proportions\n\n\nsurvey_crosstab (data ,\n                 x ,\n                 y ,\n                 z = NULL,\n                 weight= NULL,\n                 output = NULL,\n                 table_name = NULL)\n\n\nArguments\ndata Data to create the crosstab from\nx The column name for the independent variable. Must be in quotes.\ny The column name for the dependent variable. Must be in quotes.\nz OPTIONAL FOR 3 WAY CROSSTABS - The column name for the second control variable. Must be in quotes.\nweight OPTIONAL - The name of the column that contains the weights. If NULL, table will be unweighted.\noutput OPTIONAL FOR SINGLE VARIABLES - The type of output you want from the function. The available options are NULL, “download” and “print”.\n\nNULL is the default and it allows you to save the table to the environment when you use the function for a single variable.\n\"print\" prints the frequency tables in the console - can be used for single or multiple variables.\n\"download\" downloads a xlsx with the frequency table of the specified variable and weights - can be used for single or multiple variables.\n\ntable_name OPTIONAL - A string of the name you want for the table if you use the “download” output option. If NULL, then the table downloaded will be called “crosstab_table.xlsx”.\n\n\nExamples\n\n#load in the custom functions\nsource(\"00_packages_and_functions.R\")\n\n\n#create data with codes to be labelled\n\n#make the example data reproducible \nset.seed(123)\ndf &lt;- data.frame(\n  #create three variables \n  var_1= c(1,1,2,3,4,4,3,1),\n  var_2=c(1,1,2,1,2,1,2,1),\n  var_3 = c(2,3,1,1,2,2,2,3),\n  #create weight \n  weight= stats::runif(8, min = 0.1, max = 1))\n\n\n#create a weighted crosstab\n\nsurvey_crosstab(df,\n                x = \"var_1\",\n                y= \"var_2\",\n                weight = \"weight\")\n\n#create an unweighted crosstab\n\nsurvey_crosstab(df,\n                x = \"var_1\",\n                y= \"var_2\")\n\n#create a weighted crosstab across multiple variables \n#print it in the console \n\nsurvey_crosstab(df,\n                x = \"var_1\",\n                y= c(\"var_2\",\"var_3\"),\n                weight = \"weight\", \n                output = \"print\")\n\n#create a weighted crosstab across multiple variables \n#download it as an Excel document \nsurvey_crosstab(df,\n                x = \"var_1\",\n                y= c(\"var_2\",\"var_3\"),\n                weight = \"weight\", \n                output = \"download\")\n\n#create a weighted 3-way crosstab \n\nsurvey_crosstab(df,\n                x = \"var_1\",\n                y= \"var_2\",\n                z=\"var_3\",\n                weight = \"weight\")",
    "crumbs": [
      "Custom functions reference"
    ]
  },
  {
    "objectID": "custom_functions.html#creating-summary-tables",
    "href": "custom_functions.html#creating-summary-tables",
    "title": "Custom functions reference",
    "section": "Creating summary tables",
    "text": "Creating summary tables\nCreate weighted and unweighted summary tables with the following columns:\n\nthe variable name\nunweighted observations\nweighted observations - weighted tables only\nweighted mean or mean\nminimum value\nmaximum value\nunweighted missing values\nweighted missing values - weighted tables only\n\n\ncreate_summary_table()\n\ncreate_summary_table (data,\n                      variables,\n                      weight=NULL,\n                      output = NULL,\n                      table_name = NULL)\n\n\nArguments\ndata Data to create the summary table from\nvariables The column name(s) for variable(s) you want the summary table for. Must be in quotes. These need to be numeric and not categorical data.\nweight OPTIONAL - The name of the column that contains the weights. If NULL, table will be unweighted.\noutput OPTIONAL FOR SINGLE VARIABLES - The type of output you want from the function. The available options are NULL, “download” and “print”.\n\nNULL is the default and it allows you to save the table to the environment when you use the function for a single variable.\n\"print\" prints the frequency tables in the console - can be used for single or multiple variables.\n\"download\" downloads a xlsx with the frequency table of the specified variable and weights - can be used for single or multiple variables.\n\ntable_name OPTIONAL - A string of the name you want for the table if you use the “download” output option. If NULL, then the table downloaded will be called “summary_table.xlsx”.\n\n\nExamples\n\n#load in the custom functions\nsource(\"00_packages_and_functions.R\")\n\n\n#create data with codes to be labelled\n\n#make the example data reproducible \nset.seed(123)\ndf &lt;- data.frame(\n  #create two variables \n  var_1= c(1,1,2,3,4,4,3,1),\n  var_2=c(1,1,2,1,2,1,2,1),\n  #create weight \n  weight= stats::runif(8, min = 0.1, max = 1))\n\n#create a weighted summary table \n\ncreate_summary_table(df,\n                 variables = \"var_1\",\n                 weight = \"weight\")\n\n#create an unweighted summary table \n\ncreate_summary_table(df,\n                 variables = \"var_1\")\n\n#create a weighted summary table across multiple variables \n#print it in the console \n\ncreate_summary_table(df,\n                 variables = c(\"var_1\",\"var_2\"),\n                 weight = \"weight\", \n                 output = \"print\")\n\n#create a weighted summary table across multiple variables \n#download it as an Excel document \ncreate_summary_table(df,\n                 variables = c(\"var_1\",\"var_2\"),\n                 weight = \"weight\", \n                 output = \"download\")",
    "crumbs": [
      "Custom functions reference"
    ]
  },
  {
    "objectID": "custom_functions.html#calculating-weighted-averages",
    "href": "custom_functions.html#calculating-weighted-averages",
    "title": "Custom functions reference",
    "section": "Calculating weighted averages",
    "text": "Calculating weighted averages\nCalculate weighted means and medians.\n\nweighted_avg()\n\nweighted_avg (data,\n              x,\n              weight,\n              type = \"mean\",\n              by = NULL,\n              ...)\n\n\nArguments\ndata Data to create the weighted average from\nx The column you want to get the average for. Must be in quotes.\nweight The name of the column that contains the weights. Must be in quotes.\ntype The type of average. One of “mean”, “median” and “avg by grp”. Default is \"mean\".\nby OPTIONAL FOR GROUPED MEAN - the name of the column you want to group the average by. Must be in quotes.\n... extra arguments to pass on to the functions used in this custom function.\nCheck documentation for survey::svymean(), survey::svyby() and survey::svyquantile() to find out more about the extra arguments that can be passed to this function.\n\n\nExamples\n\n#load in the custom functions\nsource(\"00_packages_and_functions.R\")\n\n\n#create data with codes to be labelled\n\n#make the example data reproducible \nset.seed(123)\ndf &lt;- data.frame(\n  #create two variables \n  var_1= c(1,1,2,3,4,4,3,1),\n  var_2=c(1,1,2,1,2,1,2,1),\n  #create weight \n  weight= stats::runif(8, min = 0.1, max = 1))\n\n\n#calculate weighted mean \nweighted_avg(df,\n             x=\"var_1\",\n             weight = \"weight\")\n\n#calculate weighted mean by group \nweighted_avg(df,\n             x=\"var_1\",\n             by=\"var_2\",\n             weight = \"weight\",\n             type=\"avg by grp\")\n\n#calculate weighted median\nweighted_avg(df,\n             x=\"var_1\",\n             weight = \"weight\",\n             type = \"median\",\n             ci=FALSE,\n             quantile=0.5)",
    "crumbs": [
      "Custom functions reference"
    ]
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Help and contact us",
    "section": "",
    "text": "General tips",
    "crumbs": [
      "Help and contact us"
    ]
  },
  {
    "objectID": "help.html#general-tips",
    "href": "help.html#general-tips",
    "title": "Help and contact us",
    "section": "",
    "text": "Help with general R coding\nIf you need more help to build your R coding skills, we recommend you try this Introduction to R course that was created by colleagues at DfT.\n\n\nCustom functions reference page\nIf you are using any of the custom functions in this guide and you want more help or information, refer to the custom functions reference page.\n\n\nThe search bar\nYou can use the search bar at the side of this guide to type in a keyword about what you’re looking for. For example, type frequency table and it will show clickable results of where this keyword appears throughout the guide.",
    "crumbs": [
      "Help and contact us"
    ]
  },
  {
    "objectID": "help.html#where-do-i-get-the-scripts-and-the-functions-referenced-throughout-this-guide",
    "href": "help.html#where-do-i-get-the-scripts-and-the-functions-referenced-throughout-this-guide",
    "title": "Help and contact us",
    "section": "Where do I get the scripts and the functions referenced throughout this guide?",
    "text": "Where do I get the scripts and the functions referenced throughout this guide?\nFollow the instructions on the Introductions page for downloading the materials to get the scripts required for this guide.",
    "crumbs": [
      "Help and contact us"
    ]
  },
  {
    "objectID": "help.html#common-errors-with-packages-and-functions",
    "href": "help.html#common-errors-with-packages-and-functions",
    "title": "Help and contact us",
    "section": "Common errors with packages and functions",
    "text": "Common errors with packages and functions\n\nErrors with R version\nIf you have the latest version of R downloaded but RStudio still shows an older one, use this guide to help you switch to the version of R you need.\n\n\nErrors that contain loadNamespace\nIf you get error messages with loadNamespace in them, it means that you do not have the required package, or the correct version of the package installed.\nPlease refer to the renv section of this guide and follow the instructions to solve this error.\n\n\nErrors that contain could not find function\nIf you get error messages with could not find function in them, it means that you have not loaded in the packages and functions required for the code to run.\nPlease refer to the Loading in packages and functions section of this guide and follow the instructions to solve this error.\n\n\nErrors that contain object &lt;insert object name&gt; not found\nThese errors can be due to either not reading data or not putting column names in quotes.\n\nNot reading data in\nYou are trying to use a function on data that has not been read in. To solve this, make sure you read in the data required for the function.\n\n\nNot quoting column names\nYou did not use quotes for column names in some functions. It’s important to do this because otherwise the functions are looking for objects in the environment with those names instead of using variables in the data set with that name. To solve this, use quoted names for the appropriate functions. Refer to the custom functions reference page for help or to the documentation of the specific function you’re using.\n\n\n\nErrors that contain '\\U' used without hex digits in character string\nThis error occurs when you attempt to read a file into R and use backslashes ( \\ ) in the file path. Use forward slashes ( / ) in the file path instead.\n\n\nHow do I know which parts to change in a function to suit my analysis?\nThe best way to do this is to read the documentation for those functions. You can find documentation for the custom functions in this guide in the custom functions page. You can find documentation for other functions by typing the function name and the package it comes from in Google or by typing ??package_name::function() in the console and the documentation will appear in the Help pane in the right hand side.\n\n\nOther errors\nIf you are getting other errors not mentioned above, we recommend you try the following:\n\nGoogle and Stack Overflow\nCopy and paste the error into Google and try to find a solution there. Stack Overflow is a site that is good for troubleshooting so if you see results that come up with your error from Stack Overflow, we recommend you look at them first.\n\n\nDocumentation\nCheck the documentation of the function that you’re getting an error for. You can either Google the function name to get the documentation or type ??package_name::function_name() in the console to see the documentation in the Help pane of RStudio.\n\n\nCopilot and ChatGPT\n\n\n\n\n\n\nWarning\n\n\n\nDo NOT enter sensitive information into Copilot, ChatGPT or similar websites or apps.\nDo NOT take everything that is generated by AI as accurate, as the code can be incorrect. This can have unwanted effects on your analysis.\n\n\nIf you are stuck and none of the above worked, you can use Copilot and ChatGPT to help you solve the error. You can do this by putting the error message into them and asking them how to solve the error or to write sample code that would not generate this error.",
    "crumbs": [
      "Help and contact us"
    ]
  },
  {
    "objectID": "help.html#contact-us",
    "href": "help.html#contact-us",
    "title": "Help and contact us",
    "section": "Contact us",
    "text": "Contact us\nIf you are experiencing issues and you need to get in touch with us, please email Christopher.HANLEY@education.gov.uk.",
    "crumbs": [
      "Help and contact us"
    ]
  }
]